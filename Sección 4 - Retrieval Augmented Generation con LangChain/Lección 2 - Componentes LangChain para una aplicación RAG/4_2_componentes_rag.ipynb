{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Componentes de una aplicaci√≥n RAG\n",
    "\n",
    "**Sumario**\n",
    "\n",
    "1. Introducci√≥n\n",
    "<br></br>\n",
    "1. Document loaders\n",
    "   1. ¬øPor qu√© utilizarlos?\n",
    "   2. Tipos\n",
    "   3. Document loaders personalizados\n",
    "   4. Ejemplos\n",
    "<br></br>\n",
    "1. Text splitters\n",
    "   1. ¬øPor qu√© utilizarlos?\n",
    "   2. Tipos\n",
    "   3. Ejemplos\n",
    "<br></br>\n",
    "1. Modelo de embeddings\n",
    "   1. OpenAI\n",
    "   2. HuggingFace\n",
    "<br></br>\n",
    "1. Bases de datos vectoriales\n",
    "   1. Ejemplo con Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Introducci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_4_2/rag_arquitectura.png\" width=\"1000\"/></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Document loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los \"document loaders\" (cargadores de documentos) son componentes dise√±ados para facilitar la carga, manipulaci√≥n y gesti√≥n eficiente de documentos.\n",
    "\n",
    "LangChain proporciona una serie de document loaders integrados para cargar datos de texto desde fuentes comunes, como archivos, bases de datos y APIs. Tambi√©n es posible crear document loaders personalizados para cargar datos de fuentes espec√≠ficas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - ¬øPor qu√© utilizarlos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Aislamiento de la fuente de datos:** Nos permiten separar las fuentes de datos de la aplicaci√≥n NLP.\n",
    "\n",
    "* **Flexibilidad:** LangChain posee una larga lista de loaders ya implementados para una gran variedad de fuentes de datos.\n",
    "\n",
    "* **Eficiencia:** Los document loaders pueden optimizarse para cargar datos de texto de forma eficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Tipos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Archivos**\n",
    "* HTML\n",
    "  * [`UnstructuredHTMLLoader`](https://python.langchain.com/docs/modules/data_connection/document_loaders/html)\n",
    "  * [`WebBaseLoader`](https://python.langchain.com/docs/integrations/document_loaders/web_base)\n",
    "* CSV\n",
    "  * [`CSVLoader`](https://python.langchain.com/docs/integrations/document_loaders/csv)\n",
    "* JSON\n",
    "  * [`JSONLoader`](https://python.langchain.com/docs/modules/data_connection/document_loaders/json)\n",
    "* PDF\n",
    "  * [`PyPDFLoader`](https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf#using-pypdf)\n",
    "  * [`PDFMinerLoader`](https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf#using-pdfminer)\n",
    "  * [`PyMuPDFLoader`](https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf#using-pymupdf)\n",
    "* etc.\n",
    "\n",
    "**Bases de datos**\n",
    "* Google BigQuery\n",
    "  * [`BigQueryLoader`](https://python.langchain.com/docs/integrations/document_loaders/google_bigquery)\n",
    "* AWS S3\n",
    "  * [`S3FileLoader`](https://python.langchain.com/docs/integrations/document_loaders/aws_s3_file)\n",
    "  * [`S3DirectoryLoader`](https://python.langchain.com/docs/integrations/document_loaders/aws_s3_directory)\n",
    "* Mongo DB\n",
    "  * [`MongodbLoader`](https://python.langchain.com/docs/integrations/document_loaders/mongodb)\n",
    "* Pandas DataFrame\n",
    "  * [`DataFrameLoader`](https://python.langchain.com/docs/integrations/document_loaders/pandas_dataframe)\n",
    "* etc.\n",
    "\n",
    "**APIs**\n",
    "* Wikipedia\n",
    "  * [`WikipediaLoader`](https://python.langchain.com/docs/integrations/document_loaders/wikipedia)\n",
    "* GitHub\n",
    "  * [`GitHubIssuesLoader`](https://python.langchain.com/docs/integrations/document_loaders/github)\n",
    "* Arxiv\n",
    "  * [`ArxivLoader`](https://python.langchain.com/docs/integrations/document_loaders/arxiv)\n",
    "* Telegram\n",
    "  * [`TelegramchatApiLoader`](https://python.langchain.com/docs/integrations/document_loaders/telegram)\n",
    "* Twitter (X)\n",
    "  * [`TwitterTweetLoader`](https://python.langchain.com/docs/integrations/document_loaders/twitter)\n",
    "* etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2.3 - Document loaders personalizados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tambi√©n es posible crear document loaders personalizados para cargar datos de fuentes espec√≠ficas. En este caso, tenemos varios opciones:\n",
    "\n",
    "**Heredar la clase `BaseDocumentLoader`**. Es el m√©todo general, para diferentes tipos de documentos.\n",
    "\n",
    "```python\n",
    "abstract class BaseDocumentLoader implements DocumentLoader {\n",
    "  abstract load(): Promise<Document[]>;\n",
    "}\n",
    "```\n",
    "\n",
    "**Heredar la clase `TextLoader`**. Espec√≠fico para archivos de texto, pero que tengan un formato especifico.\n",
    "\n",
    "```python\n",
    "abstract class TextLoader extends BaseDocumentLoader {\n",
    "  abstract parse(raw: string): Promise<string[]>;\n",
    "}\n",
    "```\n",
    "\n",
    "**Heredar la clase `BufferLoader`**. Espec√≠fico para archivos binarios. \n",
    "La clase `BufferLoader` se encarga de leer el archivo, por lo que todo lo que tenemos que hacer es implementar el m√©todo de parseo.\n",
    "\n",
    "```python\n",
    "abstract class BufferLoader extends BaseDocumentLoader {\n",
    "  abstract parse(\n",
    "    raw: Buffer,\n",
    "    metadata: Document[\"metadata\"]\n",
    "  ): Promise<Document[]>;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 - Ejemplos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 - `PyMuPDFLoader`\n",
    "\n",
    "`pip install pymupdf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "\n",
    "pdf_loader = PyMuPDFLoader(\"data/SA_microsoft.pdf\")\n",
    "pdf_docs = pdf_loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos devuelve una lista de objetos `Document`, donde cada documento se corresponde con una p√°gina del PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pdf_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay dos claves relevantes en los documentos:\n",
    "* `page_content`. Contiene el contenido en formao string\n",
    "* `metadata`. Los metadatos del documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadatos:\n",
    "pdf_docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 - `WebBaseLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "web_loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "web_docs = web_loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descarga la pagina web completa en un solo documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(web_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si bien este loader es capaz de extraer el texto directamente a partir de una p√°gina HTML con una sola l√≠nea de c√≥digo, el texto resultante puede requerir limpieza antes de ser utilizado para entrenar o para inferencia con un LLM.\n",
    "\n",
    "Por ejemplo, en este caso como m√≠nimo deberiamos limpiar el n√∫mero de saltos de p√°gina..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 - `WikipediaLoader`\n",
    "\n",
    "`pip install wikipedia`\n",
    "\n",
    "En este caso, le hacemos una query a Wikipedia (como si usaramos la parte de \"b√∫squeda\" de la p√°gina web) y recuperamos como m√°ximo los dos primeros documentos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WikipediaLoader\n",
    "\n",
    "wiki_docs = WikipediaLoader(query=\"HUNTER X HUNTER\", load_max_docs=2).load()\n",
    "\n",
    "len(wiki_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, podemos ver que nos devuelve (a menos que cambiemos el idioma de la query intr√≠nseca) la p√°gina del manga, as√≠ como la p√°gina de los personajes principales en ingl√©s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_docs[1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Text splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los \"text splitters\" (separadores de texto) son componentes dise√±ados para dividir los textos cargados en trozos que quepan en la ventana de contexto de un modelo de embeddings o un LLM.\n",
    "\n",
    "LangChain proporciona varios text splitters para texto general, HTML y c√≥digo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - ¬øPor qu√© utilizarlos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A primer a vista, la idea de dividir el texto en trozos peque√±os pueda parecer simple. Sin embargo, hay mucho potencial de complejidad ya que por lo general **queremos mantener juntos los trozos de texto que se encuentren relacionados sem√°nticamente**. Lo que significa \"relacionado sem√°nticamente\" podr√≠a depender del tipo de texto (no es lo mismo el texto de una novela que el de un c√≥digo Python). \n",
    "\n",
    "Distinguimos dos caracter√≠sticas principales en un \"text splitter\" (separador de texto):\n",
    "* C√≥mo se divide el texto\n",
    "* C√≥mo se mide el tama√±o del trozo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - Tipos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Nombre    | Divide en                        | Agrega Metadatos | Descripci√≥n                                                                                                                              |\n",
    "|-----------|------------------------------------|-------------------|------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| [`RecursiveCharacterTextSplitter`](https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter) | Una lista de caracteres definidos por el usuario |               | Divide el texto de forma recursiva. La divisi√≥n recursiva tiene como objetivo mantener piezas de texto relacionadas entre s√≠. Esta es la forma recomendada de comenzar a dividir el texto. |\n",
    "| [`HTMLHeaderTextSplitter`](https://python.langchain.com/docs/modules/data_connection/document_transformers/HTML_header_metadata)      | Caracteres espec√≠ficos de HTML       | ‚úÖ            | Divide el texto seg√∫n caracteres espec√≠ficos de HTML. Notablemente, esto agrega informaci√≥n relevante sobre de d√≥nde provino ese fragmento (basado en HTML).   |\n",
    "| [`MarkdownHeaderTextSplitter`](https://python.langchain.com/docs/modules/data_connection/document_transformers/markdown_header_metadata)  | Caracteres espec√≠ficos de Markdown   | ‚úÖ            | Divide el texto seg√∫n caracteres espec√≠ficos de Markdown. Notablemente, esto agrega informaci√≥n relevante sobre de d√≥nde provino ese fragmento (basado en Markdown). |\n",
    "| [C√≥digo](https://python.langchain.com/docs/modules/data_connection/document_transformers/code_splitter)    | Caracteres espec√≠ficos de c√≥digo (e.g., Python) |               | Divide el texto seg√∫n caracteres espec√≠ficos de lenguajes de programaci√≥n. Hay disponibles 15 lenguajes diferentes para elegir.           |\n",
    "| [Token](https://python.langchain.com/docs/modules/data_connection/document_transformers/split_by_token)     | Tokens                             |               | Divide el texto en tokens. Existen m√∫ltiples formas diferentes de medir los tokens.                                                        |\n",
    "| [`CharacterTextSplitter`](https://python.langchain.com/docs/modules/data_connection/document_transformers/character_text_splitter)  | Un car√°cter definido por el usuario |               | Divide el texto seg√∫n un car√°cter definido por el usuario (e.g., `\\n`). Uno de los m√©todos m√°s simples.                                               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 - Ejemplos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 - `RecursiveCharacterTextSplitter`\n",
    "\n",
    "\n",
    "Este separador de texto es el m√°s recomendado para texto gen√©rico. Intenta dividir el texto en orden hasta que los trozos sean lo suficientemente peque√±os. \n",
    "\n",
    "Est√° parametrizado por una lista de caracteres. La lista predeterminada es [`\\n\\n`, `\\n`, `\" \"`, `\"\"`]` . Esto tiene el efecto de intentar mantener todos los p√°rrafos (y luego las oraciones, y luego las palabras) juntos tanto como sea posible, ya que esos ser√≠an gen√©ricamente los trozos de texto sem√°nticamente m√°s relacionados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pdf_docs[1].page_content\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota:** En este caso, cada linea del documento PDF se ha considerado un p√°rrafo, lo que hace que el `RecursiveCharacterTextSplitter` no funcione tan bien como nos gustaria:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_4_2/problemas_fin_de_parrafo.png\" width=\"600\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "recursive_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Ponemos un chunk size muy peque√±o, simplemente para mostrar la funcionalidad\n",
    "    chunk_size=100, # caracteres m√°ximos\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = recursive_text_splitter.split_text(text)\n",
    "for i in range(0, 3):\n",
    "    print(f\"Chunk #{i}\\n\")\n",
    "    print(chunks[i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 - Dividir por tokens usando `tiktoken`\n",
    "\n",
    "`pip install tiktoken`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "tiktoken_text_splitter = TokenTextSplitter(\n",
    "    chunk_size=100, # n√∫mero m√°ximo de tokens\n",
    "    chunk_overlap=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = tiktoken_text_splitter.split_text(text)\n",
    "for i in range(0, 2):\n",
    "    print(f\"Chunk #{i}\\n\")\n",
    "    print(chunks[i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, funciona mejor porque ignora los saltos de p√°gina erroneos que confunden al anterior text splitter, pero podemos ver que corta frases a la mitad..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 - Dividir c√≥digo\n",
    "\n",
    "Langchain ofrece splitters espec√≠ficos, aunque tampoco son la panacea. Si cambiamos el chunk size, podemos ver diferencias significativas en la manera en la que divide el texto.\n",
    "\n",
    "Personalmente creo que **si necesitamos algo realmente inteligente la mejor manera de hacerlo seria con un LLM**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "\n",
    "PYTHON_CODE = \"\"\"\n",
    "def hello_world():\n",
    "    print(\"Hello, World!\")\n",
    "\n",
    "# Llama a la funcion\n",
    "hello_world()\n",
    "\"\"\"\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=50, chunk_overlap=0\n",
    ")\n",
    "python_docs = python_splitter.create_documents([PYTHON_CODE])\n",
    "python_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_text = \"\"\"\n",
    "# ü¶úÔ∏èüîó LangChain\n",
    "\n",
    "‚ö° Building applications with LLMs through composability ‚ö°\n",
    "\n",
    "## Quick Install\n",
    "\n",
    "```bash\n",
    "# Hopefully this code block isn't split\n",
    "pip install langchain\n",
    "```\n",
    "\n",
    "As an open-source project in a rapidly developing field, we are extremely open to contributions.\n",
    "\"\"\"\n",
    "\n",
    "md_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.MARKDOWN, \n",
    "    chunk_size=60, # Si incrementamos a 100, divide el texto por el c√≥digo\n",
    "    chunk_overlap=0\n",
    ")\n",
    "md_docs = md_splitter.create_documents([markdown_text])\n",
    "md_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Modelo de embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clase Embeddings es una clase dise√±ada para interactuar con modelos de embedding de texto. Hay muchos proveedores de modelos de embedding (OpenAI, Cohere, Hugging Face, etc.) - esta clase est√° dise√±ada para proporcionar una interfaz est√°ndar para todos ellos.\n",
    "\n",
    "La clase base Embeddings en LangChain proporciona dos m√©todos: \n",
    "\n",
    "* `embed_documents`. M√©todo para convertir documentos. Toma como entrada varios documentos.\n",
    "* `embed_query`. M√©todo para convertir una consulta. Toma como entrada un texto.\n",
    "\n",
    "La razon para tener dos m√©todos separados es que algunos modelos de embedding ofrecen funcionalidad distinta cuando el input es un solo texto o varios documentos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "\n",
    "# Lee la clave de API desde el archivo de configuraci√≥n\n",
    "with open('config.txt') as f:\n",
    "    config = dict(line.strip().split('=') for line in f)\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = \"https://gpt3tests.openai.azure.com/\"\n",
    "openai.api_version = \"2022-12-01\"\n",
    "openai.api_key = config.get(\"OPENAI_API_KEY\", \"\")\n",
    "\n",
    "# Nombre del despliegue en mi Azure OpenAI Studio is \"TextEmbeddingAda002\", el modelo es \"text-embedding-ada-002\"\n",
    "engine = \"TextEmbeddingAda002\"\n",
    "openai_api_version = \"2023-12-04\" \n",
    "\n",
    "openai_embeddings_model = AzureOpenAIEmbeddings(azure_endpoint=openai.api_base, azure_deployment=engine, openai_api_key=config.get(\"OPENAI_API_KEY\", \"\"), openai_api_version=openai.api_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = \"Hola Mundo\"\n",
    "embedded_query = openai_embeddings_model.embed_query(texto)\n",
    "\n",
    "print(len(embedded_query)) # n√∫mero de dimensiones del vector de embeddings resultante\n",
    "print(embedded_query[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_1 = \"Hola Mundo\"\n",
    "texto_2 = \"Hello World\"\n",
    "\n",
    "textos = [texto_1, texto_2]\n",
    "embedded_docs = openai_embeddings_model.embed_documents(textos)\n",
    "\n",
    "print(len(embedded_docs)) # Numero de vectores devueltos\n",
    "print(len(embedded_docs[0])) # dimensionalidad de los vectores\n",
    "print(embedded_docs[0][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 - API\n",
    "\n",
    "Podemos correr los modelos de forma remota mediante la API abierta de HuggingFace (aunque para temas serios tendriamos que pagar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "\n",
    "# https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "huggingface_embeddings_model = HuggingFaceInferenceAPIEmbeddings(\n",
    "    api_key=config.get(\"HUGGINGFACE_API_KEY\", \"\"),\n",
    "    model_name=\"sentence-transformers/all-MiniLM-l6-v2\"\n",
    ")\n",
    "\n",
    "embedded_query = huggingface_embeddings_model.embed_query(texto)\n",
    "print(len(embedded_query)) # n√∫mero de dimensiones del vector de embeddings resultante\n",
    "print(embedded_query[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 - Local\n",
    "\n",
    "Tambienp odemos correr el modelo de embeddings de forma local (deberia salir el mismo resultado).\n",
    "\n",
    "`pip install sentence_transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "local_huggingface_embeddings_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "embedded_query = local_huggingface_embeddings_model.embed_query(texto)\n",
    "print(len(embedded_query)) # n√∫mero de dimensiones del vector de embeddings resultante\n",
    "print(embedded_query[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Base de datos vectorial\n",
    "\n",
    "Hay gran cantidad de bases de datos vectoriales. Aqui vamos a mostrar la funcionalidad con [Chroma](https://www.trychroma.com/), una alternativa open-source que adem√°s no necesita de una API ya que funcionan de forma local (perfecto para hacer pruebas).\n",
    "\n",
    "Para explorar todas las posibilidades que ofrece LangChain, lo mejor es acudir a [la documentaci√≥n oficial.](https://python.langchain.com/docs/integrations/vectorstores)\n",
    "\n",
    "`pip install chromadb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 - Ejemplo con Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el siguiente ejemplo simple, vamos a coger el earnings-call de Microsoft que tenemos en formato PDF, dividirlo en chunks, pasar esos chunks por el modelo de embeddings y almacenarlo en una instancia de base de datos Chroma local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Cargamos el documento\n",
    "pdf_loader = PyMuPDFLoader(\"data/SA_microsoft.pdf\")\n",
    "pdf_docs = pdf_loader.load()\n",
    "\n",
    "# Dividimos el documento en chunks\n",
    "tiktoken_text_splitter = TokenTextSplitter(\n",
    "    chunk_size=100, # n√∫mero m√°ximo de tokens\n",
    "    chunk_overlap=0\n",
    ")\n",
    "chunks = tiktoken_text_splitter.split_documents(pdf_docs)\n",
    "# El numero cambia segun el chunk_size, el minimo es 21, \n",
    "# ya que es el numero de documentos iniciales, \n",
    "# correspondiente con el numero de paginas en el PDF\n",
    "print(f\"Hemos generado {len(chunks)} chunks de texto\") \n",
    "\n",
    "# Cargamos y aplicamos el modelo de embeddings\n",
    "embeddings_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Guardamos los chunks en una instancia de Chroma que reside en memoria RAM\n",
    "db = Chroma.from_documents(chunks, embeddings_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez hemos generado la Base de datos con los chunks de texto relevantes. Podemos hacer querys sobre ella de dos maneras:\n",
    "* `db.similarity_search()`. Directamente con el texto en cuestion (LangChain llama automaticamente al modelo y genera el vector de embeddings)\n",
    "* `db.similarity_search_by_vector()`. Con el vector de embeddings (habiendolo generado manualmnete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba con el texto directamente:\n",
    "query = \"What are the applications of chat-gpt in cars?\"\n",
    "docs = db.similarity_search(query)\n",
    "\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba donde primero aplicamos el modelo de embeddings \n",
    "# y luego hacemos la query sobre la Base de datos\n",
    "query = \"What are the applications of chat-gpt in cars?\"\n",
    "query_embedding_vector = embeddings_model.embed_query(query)\n",
    "docs = db.similarity_search_by_vector(query_embedding_vector)\n",
    "\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siguiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente lecci√≥n, veremos que es un agente, como utilizarlo, y sus principales usos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zero-touch-coding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
