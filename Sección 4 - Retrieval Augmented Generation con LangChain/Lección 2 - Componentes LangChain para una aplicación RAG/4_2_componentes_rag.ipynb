{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Componentes de una aplicación RAG\n",
    "\n",
    "**Sumario**\n",
    "\n",
    "1. Introducción\n",
    "<br></br>\n",
    "1. Document loaders\n",
    "   1. ¿Por qué utilizarlos?\n",
    "   2. Tipos\n",
    "   3. Document loaders personalizados\n",
    "   4. Ejemplos\n",
    "<br></br>\n",
    "1. Text splitters\n",
    "   1. ¿Por qué utilizarlos?\n",
    "   2. Tipos\n",
    "   3. Ejemplos\n",
    "<br></br>\n",
    "1. Modelo de embeddings\n",
    "   1. OpenAI\n",
    "   2. HuggingFace\n",
    "<br></br>\n",
    "1. Bases de datos vectoriales\n",
    "   1. Ejemplo con Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_4_2/rag_arquitectura.png\" width=\"1000\"/></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Document loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los \"document loaders\" (cargadores de documentos) son componentes diseñados para facilitar la carga, manipulación y gestión eficiente de documentos.\n",
    "\n",
    "LangChain proporciona una serie de document loaders integrados para cargar datos de texto desde fuentes comunes, como archivos, bases de datos y APIs. También es posible crear document loaders personalizados para cargar datos de fuentes específicas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - ¿Por qué utilizarlos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Aislamiento de la fuente de datos:** Nos permiten separar las fuentes de datos de la aplicación NLP.\n",
    "\n",
    "* **Flexibilidad:** LangChain posee una larga lista de loaders ya implementados para una gran variedad de fuentes de datos.\n",
    "\n",
    "* **Eficiencia:** Los document loaders pueden optimizarse para cargar datos de texto de forma eficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Tipos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Archivos**\n",
    "* HTML\n",
    "  * [`UnstructuredHTMLLoader`](https://python.langchain.com/docs/modules/data_connection/document_loaders/html)\n",
    "  * [`WebBaseLoader`](https://python.langchain.com/docs/integrations/document_loaders/web_base)\n",
    "* CSV\n",
    "  * [`CSVLoader`](https://python.langchain.com/docs/integrations/document_loaders/csv)\n",
    "* JSON\n",
    "  * [`JSONLoader`](https://python.langchain.com/docs/modules/data_connection/document_loaders/json)\n",
    "* PDF\n",
    "  * [`PyPDFLoader`](https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf#using-pypdf)\n",
    "  * [`PDFMinerLoader`](https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf#using-pdfminer)\n",
    "  * [`PyMuPDFLoader`](https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf#using-pymupdf)\n",
    "* etc.\n",
    "\n",
    "**Bases de datos**\n",
    "* Google BigQuery\n",
    "  * [`BigQueryLoader`](https://python.langchain.com/docs/integrations/document_loaders/google_bigquery)\n",
    "* AWS S3\n",
    "  * [`S3FileLoader`](https://python.langchain.com/docs/integrations/document_loaders/aws_s3_file)\n",
    "  * [`S3DirectoryLoader`](https://python.langchain.com/docs/integrations/document_loaders/aws_s3_directory)\n",
    "* Mongo DB\n",
    "  * [`MongodbLoader`](https://python.langchain.com/docs/integrations/document_loaders/mongodb)\n",
    "* Pandas DataFrame\n",
    "  * [`DataFrameLoader`](https://python.langchain.com/docs/integrations/document_loaders/pandas_dataframe)\n",
    "* etc.\n",
    "\n",
    "**APIs**\n",
    "* Wikipedia\n",
    "  * [`WikipediaLoader`](https://python.langchain.com/docs/integrations/document_loaders/wikipedia)\n",
    "* GitHub\n",
    "  * [`GitHubIssuesLoader`](https://python.langchain.com/docs/integrations/document_loaders/github)\n",
    "* Arxiv\n",
    "  * [`ArxivLoader`](https://python.langchain.com/docs/integrations/document_loaders/arxiv)\n",
    "* Telegram\n",
    "  * [`TelegramchatApiLoader`](https://python.langchain.com/docs/integrations/document_loaders/telegram)\n",
    "* Twitter (X)\n",
    "  * [`TwitterTweetLoader`](https://python.langchain.com/docs/integrations/document_loaders/twitter)\n",
    "* etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2.3 - Document loaders personalizados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También es posible crear document loaders personalizados para cargar datos de fuentes específicas. En este caso, tenemos varios opciones:\n",
    "\n",
    "**Heredar la clase `BaseDocumentLoader`**. Es el método general, para diferentes tipos de documentos.\n",
    "\n",
    "```python\n",
    "abstract class BaseDocumentLoader implements DocumentLoader {\n",
    "  abstract load(): Promise<Document[]>;\n",
    "}\n",
    "```\n",
    "\n",
    "**Heredar la clase `TextLoader`**. Específico para archivos de texto, pero que tengan un formato especifico.\n",
    "\n",
    "```python\n",
    "abstract class TextLoader extends BaseDocumentLoader {\n",
    "  abstract parse(raw: string): Promise<string[]>;\n",
    "}\n",
    "```\n",
    "\n",
    "**Heredar la clase `BufferLoader`**. Específico para archivos binarios. \n",
    "La clase `BufferLoader` se encarga de leer el archivo, por lo que todo lo que tenemos que hacer es implementar el método de parseo.\n",
    "\n",
    "```python\n",
    "abstract class BufferLoader extends BaseDocumentLoader {\n",
    "  abstract parse(\n",
    "    raw: Buffer,\n",
    "    metadata: Document[\"metadata\"]\n",
    "  ): Promise<Document[]>;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 - Ejemplos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 - `PyMuPDFLoader`\n",
    "\n",
    "`pip install pymupdf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "\n",
    "pdf_loader = PyMuPDFLoader(\"data/SA_microsoft.pdf\")\n",
    "pdf_docs = pdf_loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos devuelve una lista de objetos `Document`, donde cada documento se corresponde con una página del PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pdf_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay dos claves relevantes en los documentos:\n",
    "* `page_content`. Contiene el contenido en formao string\n",
    "* `metadata`. Los metadatos del documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadatos:\n",
    "pdf_docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 - `WebBaseLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "web_loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "web_docs = web_loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descarga la pagina web completa en un solo documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(web_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si bien este loader es capaz de extraer el texto directamente a partir de una página HTML con una sola línea de código, el texto resultante puede requerir limpieza antes de ser utilizado para entrenar o para inferencia con un LLM.\n",
    "\n",
    "Por ejemplo, en este caso como mínimo deberiamos limpiar el número de saltos de página..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 - `WikipediaLoader`\n",
    "\n",
    "`pip install wikipedia`\n",
    "\n",
    "En este caso, le hacemos una query a Wikipedia (como si usaramos la parte de \"búsqueda\" de la página web) y recuperamos como máximo los dos primeros documentos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WikipediaLoader\n",
    "\n",
    "wiki_docs = WikipediaLoader(query=\"HUNTER X HUNTER\", load_max_docs=2).load()\n",
    "\n",
    "len(wiki_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, podemos ver que nos devuelve (a menos que cambiemos el idioma de la query intrínseca) la página del manga, así como la página de los personajes principales en inglés:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_docs[1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Text splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los \"text splitters\" (separadores de texto) son componentes diseñados para dividir los textos cargados en trozos que quepan en la ventana de contexto de un modelo de embeddings o un LLM.\n",
    "\n",
    "LangChain proporciona varios text splitters para texto general, HTML y código."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - ¿Por qué utilizarlos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A primer a vista, la idea de dividir el texto en trozos pequeños pueda parecer simple. Sin embargo, hay mucho potencial de complejidad ya que por lo general **queremos mantener juntos los trozos de texto que se encuentren relacionados semánticamente**. Lo que significa \"relacionado semánticamente\" podría depender del tipo de texto (no es lo mismo el texto de una novela que el de un código Python). \n",
    "\n",
    "Distinguimos dos características principales en un \"text splitter\" (separador de texto):\n",
    "* Cómo se divide el texto\n",
    "* Cómo se mide el tamaño del trozo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - Tipos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Nombre    | Divide en                        | Agrega Metadatos | Descripción                                                                                                                              |\n",
    "|-----------|------------------------------------|-------------------|------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| [`RecursiveCharacterTextSplitter`](https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter) | Una lista de caracteres definidos por el usuario |               | Divide el texto de forma recursiva. La división recursiva tiene como objetivo mantener piezas de texto relacionadas entre sí. Esta es la forma recomendada de comenzar a dividir el texto. |\n",
    "| [`HTMLHeaderTextSplitter`](https://python.langchain.com/docs/modules/data_connection/document_transformers/HTML_header_metadata)      | Caracteres específicos de HTML       | ✅            | Divide el texto según caracteres específicos de HTML. Notablemente, esto agrega información relevante sobre de dónde provino ese fragmento (basado en HTML).   |\n",
    "| [`MarkdownHeaderTextSplitter`](https://python.langchain.com/docs/modules/data_connection/document_transformers/markdown_header_metadata)  | Caracteres específicos de Markdown   | ✅            | Divide el texto según caracteres específicos de Markdown. Notablemente, esto agrega información relevante sobre de dónde provino ese fragmento (basado en Markdown). |\n",
    "| [Código](https://python.langchain.com/docs/modules/data_connection/document_transformers/code_splitter)    | Caracteres específicos de código (e.g., Python) |               | Divide el texto según caracteres específicos de lenguajes de programación. Hay disponibles 15 lenguajes diferentes para elegir.           |\n",
    "| [Token](https://python.langchain.com/docs/modules/data_connection/document_transformers/split_by_token)     | Tokens                             |               | Divide el texto en tokens. Existen múltiples formas diferentes de medir los tokens.                                                        |\n",
    "| [`CharacterTextSplitter`](https://python.langchain.com/docs/modules/data_connection/document_transformers/character_text_splitter)  | Un carácter definido por el usuario |               | Divide el texto según un carácter definido por el usuario (e.g., `\\n`). Uno de los métodos más simples.                                               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 - Ejemplos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 - `RecursiveCharacterTextSplitter`\n",
    "\n",
    "\n",
    "Este separador de texto es el más recomendado para texto genérico. Intenta dividir el texto en orden hasta que los trozos sean lo suficientemente pequeños. \n",
    "\n",
    "Está parametrizado por una lista de caracteres. La lista predeterminada es [`\\n\\n`, `\\n`, `\" \"`, `\"\"`]` . Esto tiene el efecto de intentar mantener todos los párrafos (y luego las oraciones, y luego las palabras) juntos tanto como sea posible, ya que esos serían genéricamente los trozos de texto semánticamente más relacionados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pdf_docs[1].page_content\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota:** En este caso, cada linea del documento PDF se ha considerado un párrafo, lo que hace que el `RecursiveCharacterTextSplitter` no funcione tan bien como nos gustaria:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_4_2/problemas_fin_de_parrafo.png\" width=\"600\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "recursive_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Ponemos un chunk size muy pequeño, simplemente para mostrar la funcionalidad\n",
    "    chunk_size=100, # caracteres máximos\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = recursive_text_splitter.split_text(text)\n",
    "for i in range(0, 3):\n",
    "    print(f\"Chunk #{i}\\n\")\n",
    "    print(chunks[i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 - Dividir por tokens usando `tiktoken`\n",
    "\n",
    "`pip install tiktoken`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "tiktoken_text_splitter = TokenTextSplitter(\n",
    "    chunk_size=100, # número máximo de tokens\n",
    "    chunk_overlap=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = tiktoken_text_splitter.split_text(text)\n",
    "for i in range(0, 2):\n",
    "    print(f\"Chunk #{i}\\n\")\n",
    "    print(chunks[i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, funciona mejor porque ignora los saltos de página erroneos que confunden al anterior text splitter, pero podemos ver que corta frases a la mitad..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 - Dividir código\n",
    "\n",
    "Langchain ofrece splitters específicos, aunque tampoco son la panacea. Si cambiamos el chunk size, podemos ver diferencias significativas en la manera en la que divide el texto.\n",
    "\n",
    "Personalmente creo que **si necesitamos algo realmente inteligente la mejor manera de hacerlo seria con un LLM**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "\n",
    "PYTHON_CODE = \"\"\"\n",
    "def hello_world():\n",
    "    print(\"Hello, World!\")\n",
    "\n",
    "# Llama a la funcion\n",
    "hello_world()\n",
    "\"\"\"\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=50, chunk_overlap=0\n",
    ")\n",
    "python_docs = python_splitter.create_documents([PYTHON_CODE])\n",
    "python_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_text = \"\"\"\n",
    "# 🦜️🔗 LangChain\n",
    "\n",
    "⚡ Building applications with LLMs through composability ⚡\n",
    "\n",
    "## Quick Install\n",
    "\n",
    "```bash\n",
    "# Hopefully this code block isn't split\n",
    "pip install langchain\n",
    "```\n",
    "\n",
    "As an open-source project in a rapidly developing field, we are extremely open to contributions.\n",
    "\"\"\"\n",
    "\n",
    "md_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.MARKDOWN, \n",
    "    chunk_size=60, # Si incrementamos a 100, divide el texto por el código\n",
    "    chunk_overlap=0\n",
    ")\n",
    "md_docs = md_splitter.create_documents([markdown_text])\n",
    "md_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Modelo de embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clase Embeddings es una clase diseñada para interactuar con modelos de embedding de texto. Hay muchos proveedores de modelos de embedding (OpenAI, Cohere, Hugging Face, etc.) - esta clase está diseñada para proporcionar una interfaz estándar para todos ellos.\n",
    "\n",
    "La clase base Embeddings en LangChain proporciona dos métodos: \n",
    "\n",
    "* `embed_documents`. Método para convertir documentos. Toma como entrada varios documentos.\n",
    "* `embed_query`. Método para convertir una consulta. Toma como entrada un texto.\n",
    "\n",
    "La razon para tener dos métodos separados es que algunos modelos de embedding ofrecen funcionalidad distinta cuando el input es un solo texto o varios documentos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "\n",
    "# Lee la clave de API desde el archivo de configuración\n",
    "with open('config.txt') as f:\n",
    "    config = dict(line.strip().split('=') for line in f)\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = \"https://gpt3tests.openai.azure.com/\"\n",
    "openai.api_version = \"2022-12-01\"\n",
    "openai.api_key = config.get(\"OPENAI_API_KEY\", \"\")\n",
    "\n",
    "# Nombre del despliegue en mi Azure OpenAI Studio is \"TextEmbeddingAda002\", el modelo es \"text-embedding-ada-002\"\n",
    "engine = \"TextEmbeddingAda002\"\n",
    "openai_api_version = \"2023-12-04\" \n",
    "\n",
    "openai_embeddings_model = AzureOpenAIEmbeddings(azure_endpoint=openai.api_base, azure_deployment=engine, openai_api_key=config.get(\"OPENAI_API_KEY\", \"\"), openai_api_version=openai.api_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = \"Hola Mundo\"\n",
    "embedded_query = openai_embeddings_model.embed_query(texto)\n",
    "\n",
    "print(len(embedded_query)) # número de dimensiones del vector de embeddings resultante\n",
    "print(embedded_query[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_1 = \"Hola Mundo\"\n",
    "texto_2 = \"Hello World\"\n",
    "\n",
    "textos = [texto_1, texto_2]\n",
    "embedded_docs = openai_embeddings_model.embed_documents(textos)\n",
    "\n",
    "print(len(embedded_docs)) # Numero de vectores devueltos\n",
    "print(len(embedded_docs[0])) # dimensionalidad de los vectores\n",
    "print(embedded_docs[0][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 - API\n",
    "\n",
    "Podemos correr los modelos de forma remota mediante la API abierta de HuggingFace (aunque para temas serios tendriamos que pagar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "\n",
    "# https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "huggingface_embeddings_model = HuggingFaceInferenceAPIEmbeddings(\n",
    "    api_key=config.get(\"HUGGINGFACE_API_KEY\", \"\"),\n",
    "    model_name=\"sentence-transformers/all-MiniLM-l6-v2\"\n",
    ")\n",
    "\n",
    "embedded_query = huggingface_embeddings_model.embed_query(texto)\n",
    "print(len(embedded_query)) # número de dimensiones del vector de embeddings resultante\n",
    "print(embedded_query[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 - Local\n",
    "\n",
    "Tambienp odemos correr el modelo de embeddings de forma local (deberia salir el mismo resultado).\n",
    "\n",
    "`pip install sentence_transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "local_huggingface_embeddings_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "embedded_query = local_huggingface_embeddings_model.embed_query(texto)\n",
    "print(len(embedded_query)) # número de dimensiones del vector de embeddings resultante\n",
    "print(embedded_query[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Base de datos vectorial\n",
    "\n",
    "Hay gran cantidad de bases de datos vectoriales. Aqui vamos a mostrar la funcionalidad con [Chroma](https://www.trychroma.com/), una alternativa open-source que además no necesita de una API ya que funcionan de forma local (perfecto para hacer pruebas).\n",
    "\n",
    "Para explorar todas las posibilidades que ofrece LangChain, lo mejor es acudir a [la documentación oficial.](https://python.langchain.com/docs/integrations/vectorstores)\n",
    "\n",
    "`pip install chromadb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 - Ejemplo con Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el siguiente ejemplo simple, vamos a coger el earnings-call de Microsoft que tenemos en formato PDF, dividirlo en chunks, pasar esos chunks por el modelo de embeddings y almacenarlo en una instancia de base de datos Chroma local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Cargamos el documento\n",
    "pdf_loader = PyMuPDFLoader(\"data/SA_microsoft.pdf\")\n",
    "pdf_docs = pdf_loader.load()\n",
    "\n",
    "# Dividimos el documento en chunks\n",
    "tiktoken_text_splitter = TokenTextSplitter(\n",
    "    chunk_size=100, # número máximo de tokens\n",
    "    chunk_overlap=0\n",
    ")\n",
    "chunks = tiktoken_text_splitter.split_documents(pdf_docs)\n",
    "# El numero cambia segun el chunk_size, el minimo es 21, \n",
    "# ya que es el numero de documentos iniciales, \n",
    "# correspondiente con el numero de paginas en el PDF\n",
    "print(f\"Hemos generado {len(chunks)} chunks de texto\") \n",
    "\n",
    "# Cargamos y aplicamos el modelo de embeddings\n",
    "embeddings_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Guardamos los chunks en una instancia de Chroma que reside en memoria RAM\n",
    "db = Chroma.from_documents(chunks, embeddings_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez hemos generado la Base de datos con los chunks de texto relevantes. Podemos hacer querys sobre ella de dos maneras:\n",
    "* `db.similarity_search()`. Directamente con el texto en cuestion (LangChain llama automaticamente al modelo y genera el vector de embeddings)\n",
    "* `db.similarity_search_by_vector()`. Con el vector de embeddings (habiendolo generado manualmnete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba con el texto directamente:\n",
    "query = \"What are the applications of chat-gpt in cars?\"\n",
    "docs = db.similarity_search(query)\n",
    "\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba donde primero aplicamos el modelo de embeddings \n",
    "# y luego hacemos la query sobre la Base de datos\n",
    "query = \"What are the applications of chat-gpt in cars?\"\n",
    "query_embedding_vector = embeddings_model.embed_query(query)\n",
    "docs = db.similarity_search_by_vector(query_embedding_vector)\n",
    "\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siguiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente lección, veremos que es un agente, como utilizarlo, y sus principales usos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zero-touch-coding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
