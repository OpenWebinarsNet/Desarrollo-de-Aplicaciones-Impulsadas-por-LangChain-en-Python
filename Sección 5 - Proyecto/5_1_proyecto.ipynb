{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objetivo\n",
    "\n",
    "El objetivo de este notebook es extraer información relevante de archivos financieros, como son las llamadas trimestrales de ganancias de una empresa. Para ello, se plantean tres tareas principales:\n",
    "\n",
    "1. **Extracción de entidades:** Se identifican entidades como personas, lugares y empresas, así como sus atributos, como roles o relaciones.\n",
    "   1. Es una tarea de entity-extraction\n",
    "2. **Identificación de eventos relevantes:** Se identifican eventos relevantes en el texto y se clasifican como positivos o negativos.\n",
    "   1. Es una tarea que mezcla summarization con sentiment analysis\n",
    "3. **Búsqueda de información:** Se construye una pipeline para hacer preguntas al texto.\n",
    "   1. Es una tarea de Question-answering con Retrieval Augmented Generation\n",
    "\n",
    "No disponemos de un modelo especializado para este tipo de tareas ni de una gran base de datos con la que pre-entrenar un modelo. Por ello, vamos a utilizar un LLM de gran tamaño, como GPT-3, como punto de partida.\n",
    "\n",
    "En este caso, no se realizará fine-tuning del modelo, sino que se trabajará con el modelo base. Se experimentará con diferentes prompts y con el acercamiento RAG.\n",
    "\n",
    "----\n",
    "\n",
    "**Sumario**\n",
    "\n",
    "1. Preparar los datos\n",
    "   1. Cargar el PDF\n",
    "   2. Dividir en párrafos\n",
    "   3. Contar número de tokens por párrafo\n",
    "<br></br>\n",
    "2. Extraer entidades y relaciones\n",
    "   1. Preparar el prompt\n",
    "   2. Correr el prompt por el LLM\n",
    "   3. Formatear la salida\n",
    "   4. Comentarios\n",
    "<br></br>\n",
    "1. Identificar eventos relevantes\n",
    "   1. Preparar el prompt\n",
    "   2. Correr el prompt por el LLM\n",
    "   3. Formatear la salida\n",
    "   4. Comentarios\n",
    "<br></br>\n",
    "2. Obtener información mediante preguntas\n",
    "   1. Modelo de embeddings\n",
    "   2. Base de datos vectorial\n",
    "   3. Retrieval\n",
    "   4. Generation\n",
    "   5. Comentarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from langchain.llms import AzureOpenAI\n",
    "\n",
    "# Lee la clave de API desde el archivo de configuración\n",
    "with open('config.txt') as f:\n",
    "    config = dict(line.strip().split('=') for line in f)\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = \"https://gpt3tests.openai.azure.com/\"\n",
    "openai.api_version = \"2022-12-01\"\n",
    "openai.api_key = config.get(\"OPENAI_API_KEY\", \"\")\n",
    "\n",
    "# Nombre del despliegue en mi Azure OpenAI Studio is \"TextEmbeddingAda002\", el modelo es \"text-embedding-ada-002\"\n",
    "engine = \"Davinci003\"\n",
    "model = \"text-davinci-003\"\n",
    "openai_api_version = \"2023-12-01\" \n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    azure_endpoint=openai.api_base, \n",
    "    azure_deployment=engine, \n",
    "    openai_api_key=config.get(\"OPENAI_API_KEY\", \"\"), \n",
    "    openai_api_version=openai.api_version,\n",
    "    temperature=0, # Podemos poner la temperatura a 0 si queremos reducir la variabilidad de las respuestas\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Preparar los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro punto de partida es un archivo PDF con el texto de la llamada trimestral. Para utilizar este texto de manera efectiva con nuestro LLM, debemos prepararlo para que se ajuste a la longitud de contexto del modelo. Para ello, seguiremos los siguientes pasos principales:\n",
    "\n",
    "1. **Cargar el PDF.**\n",
    "\n",
    "2. **Dividir en párrafos** Este paso nos ayuda a gestionar el texto en trozos más pequeños y digeribles.\n",
    "\n",
    "3. **Contar tokens por párrafo.** Esta información es crucial para generar prompts que se ajusten a la longitud de contexto del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - Cargar el PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "import re\n",
    "\n",
    "# Cargamos el documento\n",
    "pdf_loader = PyMuPDFLoader(\"data/growth_point_properties.pdf\")\n",
    "pdf_docs = pdf_loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - Dividir en párrafos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso voy a utilizar un método propio para dividir el texto en vez de utilizar un textsplitter de LangChain. La razón es que el texto parece estar mal formateado y un splitter normal no funcionaria como yo quiero.\n",
    "\n",
    "**Para identificar párrafos, usamos una expresion linear que divide el texto cuando se encuentra un salto de línea `\\n` seguido de una palabra mayúscula.**\n",
    "\n",
    "Una mejor alternativa, sobretodo si vamos a tratar con múltiples documentos, sería preformattear el texto o buscar fuentes alternativas del mismo que si esten bien formateadas. ein embargo, vamos a considerar este ejercicio como una \"Prueba de Concepto\" y por ello, vamos a seguir un acercamiento más simple, trabajando directamente sobre el documento proporcionado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_paragraphs_from_pdf(pdf_docs):\n",
    "    paragraphs = []\n",
    "\n",
    "    # Iteramos por cada página del PDF\n",
    "    for pdf_page in pdf_docs:\n",
    "        page_text = pdf_page.page_content\n",
    "\n",
    "        # Usamos una expresión regular para identificar parrafos como aquellas lineas que empiezan por \"\\n\" y luego una letra mayúscula\n",
    "        page_paragraphs = re.split(r'\\n(?=[A-Z])', page_text)\n",
    "\n",
    "        # Eliminamos parrafos vacios\n",
    "        page_paragraphs = [p.strip() for p in page_paragraphs if p.strip()]\n",
    "\n",
    "        # Añadimos el parrafo a la lista\n",
    "        paragraphs.extend(page_paragraphs)\n",
    "\n",
    "    return paragraphs\n",
    "\n",
    "# Dividimos el PDF en párrafos\n",
    "paragraphs = extract_paragraphs_from_pdf(pdf_docs)\n",
    "\n",
    "# Ahora \"paragraphs\" es una lista de strings, donde cada string es un párrafo del PDF (aproximadamente). Por ello, eliminamos los saltos de linea \"ignorados\"\n",
    "cleaned_paragraphs = [paragraph.replace(\"\\n\", \"\") for paragraph in paragraphs]\n",
    "\n",
    "# Ahora añadimos un salto de pagina al inicio de cada párrafo para cuando utilicemos esta informacion en los prompts\n",
    "cleaned_paragraphs = [\"\\n\" + paragraph for paragraph in paragraphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 - Contar el número de tokens por párrafo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro objetivo es pasar bloques de texto al LLM para que identifique entidades. Sin embargo, dentro del texto que vamos a pasar al LLM en forma de prompt va a haber información sobre como queremos que se extraigan. Es por ello que tenemos que medir bien los tamaño para que encajen dentro de la ventana de contexto.\n",
    "\n",
    "**Si ajustamos bien el texto, podemos reducir el número de llamadas que hacemos al modelo, y por tanto reducir la latencia de nuestro sistema.**\n",
    "\n",
    "Además saber cuantos tokens hay en cada párrafo nos permite saber cuanto cuesta (aproximadamente) cada llamada al modelo ya que en casos como el de GPT-3, el coste de las llamadas depende del número de tokens utilizados.\n",
    "\n",
    "| Nombre del encoding     | Modelos OpenAI                                       |\n",
    "|-------------------------|-----------------------------------------------------|\n",
    "| `cl100k_base`           | `gpt-4`, `gpt-3.5-turbo`, `text-embedding-ada-002`  |\n",
    "| `p50k_base`             | Codex models, `text-davinci-002`, `text-davinci-003`|\n",
    "| `r50k_base` (or `gpt2`) | GPT-3 models like `davinci`                         |\n",
    "\n",
    "Podemos saber cual es el encoding de un modelo llamando al metodo `tiktoken.encoding_for_model()`:\n",
    "\n",
    "```python\n",
    "encoding = tiktoken.encoding_for_model('text-davinci-003')\n",
    "```\n",
    "\n",
    "[Ejemplos sobre como contar tokens con `tiktoken`](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import pandas as pd\n",
    "\n",
    "def count_tokens(encoding, text):\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(model) # \"text-davinci-003\"\n",
    "\n",
    "# Count tokens for each paragraph and store the results in a list of dictionaries\n",
    "token_counts = []\n",
    "for paragraph in cleaned_paragraphs:\n",
    "    num_tokens = count_tokens(encoding, paragraph)\n",
    "    token_counts.append({'Paragraph': paragraph, 'Token Count': num_tokens})\n",
    "\n",
    "# Create a Pandas DataFrame from the list of token counts\n",
    "df = pd.DataFrame(token_counts)\n",
    "print(f\"Número total de tokens en el archivo: {df['Token Count'].sum()}\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Extraer entidades y relaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta tarea, combinamos las tareas NLP de \"entity-extraction\" y \"relation-extraction\" mediante un enfoque generativo. \n",
    "\n",
    "* **Entity-extraction** (extracción de entidades). Se refiere a la identificación y clasificación de entidades mencionadas en el texto, como nombres de personas, lugares , organizaciones, etc.\n",
    "* **Relation-extraction** (extracción de relaciones). Implica identificar y clasificar las relaciones semánticas entre entidades previamente extraidas. Por ejemplo, en una oración como \"Bill gates es fundador de Microsoft\", identificariamos la relación \"fundador\" entre las entidades \"Bill Gates\" y \"Microsoft\". Otras posibles relaciones que se pueden extraer indirectamente son: \"Bill Gates es emprendedor\", \"Bill Gates es empresario\", etc.\n",
    "\n",
    "Para lograrlo, utilizaremos [GPT-3](https://platform.openai.com/docs/models/gpt-3-5) con Langchain, empleando una [LLMChain](https://docs.langchain.com/docs/components/chains/llm-chain). Como ya hemos visto, la `LLMChain`  se compone de un PromptTemplate, un modelo y opcionalmente un formateador del output.\n",
    "\n",
    "Nuestro objetivo implica identificar los siguientes elementos:\n",
    "\n",
    "* Entidades, como **individuos** y **empresas**.\n",
    "* Atributos pertinentes asociados con estas entidades, como **roles** o **relaciones**.\n",
    "\n",
    "Nos centraremos principalmente en tres tipos de relaciones que existen entre entidades:\n",
    "\n",
    "1. `<is_a>`: Esta relación es útil para definir la naturaleza de empresas, lugares o activos.\n",
    "2. `<works_at>`: Esta relación indica el lugar de empleo de una persona.\n",
    "3. `<has_position>`: Esta relación especifica el rol o posición que ocupa una persona dentro de la empresa con la que está asociada.\n",
    "\n",
    "**Nota:** Si bien este enfoque podria ampliarse a otras relaciones más complejas, hemos optado por centrarnos en estas tres por claridad y con fines de demonstración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_relationships_task = \"entity_relationships\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Preparar el prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos dividio el prompt en 3 partes, cada una de ellas se encuentra en un archivo de texto:\n",
    "\n",
    "* `base_template.txt`. Tiene el texto base que da contexto al modelo sobre lo que le vamos a pedir. Basicamente le indicamos que queremos hacer una tarea de extracción, donde las relaciones que encuentre el modelo en el texto deben ser indicadas en el formato `ENTITY_1 <RELATIONSHIP> ENTITY_2`.\n",
    "* `pcpa.txt`. Indica el tipo de relaciones que nos interesan, y provee al modelo de explicaciones sobre cada una de ellas, para darle contexto.\n",
    "* `pcpa_microsoft`. Contiene un ejemplo hecho a mano (one-shot learning) donde a partir de un texto, le indicamos que relaciones deberia extraer del mismo. \n",
    "\n",
    "La idea de este prompt es explicar al LLM lo mejor posible la tarea para que pueda extraer correctamente entidades en el formato especificado. Como no hemos hecho ningún tipo de fine-tuning, hay que hacer algo de \"prompt engineering\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Ruta al archivo base de plantilla para tareas de extracción de entidades\n",
    "base_template_path = Path(f\"./prompts/templates/{entity_relationships_task}/base_template.txt\")\n",
    "\n",
    "# Ruta al archivo que muestra las relaciones que nos interesan extraer\n",
    "relationships_template_path = Path(f\"./prompts/templates/{entity_relationships_task}/pcpa.txt\")\n",
    "\n",
    "# Ejemplo de indicación para las relaciones específicas definidas anteriormente\n",
    "example_prompt_path = Path(f\"./prompts/examples/{entity_relationships_task}/pcpa_microsoft.txt\")\n",
    "\n",
    "# Leer el contenido de la plantilla base desde el archivo\n",
    "with open(base_template_path, 'r') as prompt_file:\n",
    "    base_template = prompt_file.read()\n",
    "\n",
    "# Leer el contenido de la plantilla de relaciones desde el archivo\n",
    "with open(relationships_template_path, 'r') as prompt_file:\n",
    "    relationships_template = prompt_file.read()\n",
    "\n",
    "# Leer el contenido del ejemplo de indicación desde el archivo\n",
    "with open(example_prompt_path, 'r') as prompt_file:\n",
    "    example_prompt = prompt_file.read()\n",
    "\n",
    "# Combinar el texto del ejemplo con el template del tipo de relaciones que nos interesan\n",
    "prompt_text = relationships_template.format(example=example_prompt)\n",
    "\n",
    "# Combinar el texto de la plantilla base con el prompt anterior\n",
    "prompt_text = base_template.format(prompt=prompt_text)\n",
    "\n",
    "# Contar el número de tokens en el texto del prompt\n",
    "n_prompt_tokens = count_tokens(encoding, prompt_text)\n",
    "print(n_prompt_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparar chunks de texto a analizar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el texto anterior ya tenemos el contexto necesario para el LLM. Ahora lo que tenemos que hacer es preparar los chunks de texto de entrada para el modelo.\n",
    "\n",
    "En este sentido, es muy importante adecuar de forma correcta el tamaño de los chunks, ya que si nos pasamos de tamaño el modelo nos dará un error (o no generará todo lo necesario). Hay 4 variables a considerar cuando decidimos el tamaño de los chunks:\n",
    "\n",
    "* **El contexto máximo del modelo** (en numero de tokens). En el caso de GPT-3 es 4096. **Esto incluye tanto entrada como generación.**\n",
    "* **El número máximo de tokens que damos de margen al modelo para generar.**\n",
    "\n",
    "Además de estos dos, hay que tener que el prompt base ya tiene un numero de tokens (899) y que `tiktoken` no es deterministico (lo he observado empíricamente pero no he investigado la razón). Por ello el número **máximo de tokens del chunk** se puede estimar con la siguiente fórmula:\n",
    "\n",
    "\n",
    "```python\n",
    "max_tokens_per_chunk = MODEL_CONTEXT_LENGTH - MAX_GENERATION_LENGTH - EXTRA_TOKENS_FOR_TOKENIZATION_VARIABILITY - n_prompt_tokens\n",
    "max_tokens_per_chunk = 4097 - 1000 - 50 - 899\n",
    "max_tokens_per_chunk = 2148\n",
    "```\n",
    "\n",
    "**Cómo máximo vamos a permitir chunks de 2147 tokens**. Además, como nuestro objetivo es identificar entidades, **vamos a evitar cortar párrafos por la mitad** ya que podria afectar a las entidades o las relaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONTEXT_LENGTH = 4097\n",
    "MAX_GENERATION_LENGTH = 1000\n",
    "EXTRA_TOKENS_FOR_TOKENIZATION_VARIABILITY = 50 # Not sure why, but I have run the same process multiple times and I have seen different tokenizations, need to double check this\n",
    "llm.max_tokens = MAX_GENERATION_LENGTH\n",
    "\n",
    "# Número máximo de tokens por chunk\n",
    "max_tokens_per_chunk = MODEL_CONTEXT_LENGTH - MAX_GENERATION_LENGTH - EXTRA_TOKENS_FOR_TOKENIZATION_VARIABILITY - n_prompt_tokens\n",
    "\n",
    "print(f\"Número máximo de tokens por chunk: {max_tokens_per_chunk}\")\n",
    "\n",
    "def generate_chunks(dataframe, max_tokens_per_chunk):\n",
    "    # Inicializar variables\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_token_count = 0\n",
    "\n",
    "    # Iterar a través de las filas del DataFrame, que son parrafos del texto\n",
    "    for index, row in dataframe.iterrows():\n",
    "        paragraph = row['Paragraph']\n",
    "        num_tokens = row['Token Count']\n",
    "\n",
    "        # Si agregar el párrafo actual al fragmento actual no excede el límite\n",
    "        if current_token_count + num_tokens <= max_tokens_per_chunk:\n",
    "            current_chunk.append(paragraph)\n",
    "            current_token_count += num_tokens\n",
    "        else:\n",
    "            # Agregar el parrafo actual a la lista de chunks\n",
    "            if current_chunk:\n",
    "                chunks.append(\"\".join(current_chunk))\n",
    "            # Comenzar un nuevo chunk con el párrafo actual\n",
    "            current_chunk = [paragraph]\n",
    "            current_token_count = num_tokens\n",
    "\n",
    "    # Agregar el último chunk (si existe)\n",
    "    if current_chunk:\n",
    "        chunks.append(\"\".join(current_chunk))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = generate_chunks(df, max_tokens_per_chunk)\n",
    "\n",
    "total_token_count = 0\n",
    "for i in range(0, len(chunks)):\n",
    "    token_count = count_tokens(encoding, chunks[i])\n",
    "    print(f\"{i}: {token_count}\")\n",
    "    total_token_count += token_count\n",
    "\n",
    "print(f\"Total: {total_token_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, se han generado 5 chunks, donde ninguno de ellos excede el máximo de tokens por chunk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Correr el prompt por el LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "def llm_run(chain, query):\n",
    "    start_time = time.time()  # Registrar el tiempo de inicio\n",
    "\n",
    "    with get_openai_callback() as cb:\n",
    "        result = chain.run(query)\n",
    "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
    "\n",
    "    end_time = time.time()  # Registrar el tiempo de finalización\n",
    "    execution_time = end_time - start_time  # Calcular el tiempo de ejecución en segundos\n",
    "    print(f'Time taken: {round(execution_time, 2)} seconds')\n",
    "\n",
    "    return result\n",
    "\n",
    "# Crear la plantilla del prompt\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"input_text\"],\n",
    "    template=prompt_text + \"\\n{input_text}\",\n",
    ")\n",
    "\n",
    "# Crear la cadena LLM simple\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Ejecutar los prompts a través del LLM\n",
    "er_outputs = []\n",
    "for i in range(len(chunks)):\n",
    "    print(f\"Chunk {i}\")\n",
    "    output = llm_run(llm_chain, chunks[i])\n",
    "    er_outputs.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "er_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si bien el modelo nos devuelve tripletas de `\"Entidad_1\" <relacion> \"Entidad_2\"`, lo hace en forma de bloque de texto. Para poder utilizar dichas relaciones y almacenarlas por ejemplo en una base de datos en grafo como Node4J, tenemos que primero formatearlas correctamente..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(er_outputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - Formatear la salida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividimos aquellas lineas de texto que se encuentran en formato `\"Entidad_1\" <relacion> \"Entidad_2\"` y las almacenamos en un DataFrame de Pandas. Por supuesto, aquellas que no cumplen con las reglas son ignoradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_relationship_text(text):\n",
    "    # Dividir el texto basándose en \"<\" y \">\" para separar la parte de la relación\n",
    "    parts = text.split(\"<\")\n",
    "    \n",
    "    if len(parts) == 2:\n",
    "        # Extraer entidades y relación\n",
    "        entity1 = parts[0].strip()\n",
    "        relationship, entity2 = parts[1].split(\">\")\n",
    "        entity2 = entity2.strip()\n",
    "        return entity1, relationship, entity2\n",
    "    else:\n",
    "        # Manejar de manera elegante la entrada no válida\n",
    "        print(f\"Formato no válido para: {text}\")\n",
    "        return None, None, None\n",
    "\n",
    "er_output_dfs = []\n",
    "\n",
    "# Crear DataFrames para cada salida en er_outputs\n",
    "for output in er_outputs:\n",
    "    er_output_dfs.append(pd.DataFrame({\"Triplet\": output.split(\"\\n\")}))\n",
    "\n",
    "# Concatenar los DataFrames\n",
    "er_output_df = pd.concat(er_output_dfs)\n",
    "\n",
    "# Eliminar filas vacías y duplicados basados en la columna \"Triplet\"\n",
    "er_output_df = er_output_df[er_output_df[\"Triplet\"] != \"\"]\n",
    "er_output_df = er_output_df.drop_duplicates([\"Triplet\"]).reset_index(drop=True).copy()\n",
    "\n",
    "# Aplicar la función al DataFrame y crear nuevas columnas\n",
    "er_output_df[['Entidad 1', 'Relación', 'Entidad 2']] = er_output_df['Triplet'].apply(lambda x: pd.Series(split_relationship_text(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "er_output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 - Comentarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar, el modelo encuentra dificultades debido a la ausencia de relaciones complejas bien definidas entre entidades, y la falta de una ontología adecuada agrava este problema. En consecuencia, el modelo ocasionalmente asigna incorrectamente relaciones `<works_at>` a empresas y lugares cuando, idealmente, debería utilizar `<is_located_at>` o una relación similar. Esta confusión surge de la limitada flexibilidad del modelo, que se debe a nuestra falta de proporcionar restricciones claras.\n",
    "\n",
    "Una mejora sencilla es **establecer restricciones que dicten que ciertos tipos de relaciones solo deben existir entre tipos específicos de entidades**. Por ejemplo, podemos marcar como incorrectas las relaciones `<works_at>` que conectan una pareja de entidades que no sean `(Persona, Compañía)`.\n",
    "\n",
    "La versión compleja seria **proporcionar una ontología que defina claramente los tipos de entidades y las relaciones posibles entre ellas**. Podriamos preparar un proceso iterativo donde las relaciones se van refinando. Tambien estaria bien contar con información extra de las entidades (por ejemplo para saber que realmente Microsoft en cierto contexto es una Compañia, pero en otro puede referise a la marca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Identificar eventos relevantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta tarea es una combinación de **resumen** y **clasificación**:\n",
    "1. \"Resumimos\" el texto extrayendo eventos relevantes\n",
    "2. Clasificamos los eventos como positivos o negativos. \n",
    "\n",
    "Para ello, utilizaremos [GPT-3](https://platform.openai.com/docs/models/gpt-3-5) con Langchain, empleando una [LLMChain](https://docs.langchain.com/docs/components/chains/llm-chain). Como ya hemos visto, la `LLMChain`  se compone de un PromptTemplate, un modelo y opcionalmente un formateador del output.\n",
    "\n",
    "**Nota:** También podríamos probar un enfoque diferente donde dividimos el proceso en dos partes y utilizamos [SimpleSequentialChain](https://python.langchain.com/docs/modules/chains/foundational/sequential_chains), donde la salida de un paso es la entrada al siguiente. **Esto nos permitiría utilizar modelos diferentes para diferentes partes del proceso**. Sería el enfoque que habría elegido si tuviera que trabajar con LLMs menos potentes (también más económicos), sobretodo si contase con más dato para entrenarlos.\n",
    "\n",
    "* La primera cadena generaría una lista de eventos.\n",
    "* La segunda cadena tomaría la lista de eventos como entrada y los clasificaría como positivos o negativos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_developments_task = \"business_developments\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - Preparar el prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso vamos a trabajar con un template más simple que en la tarea anterior ya que no vamos a proveer de ejemplos (zero-shot learning). En este caso simplemente proveemos de un texto explcativo para el modelo.\n",
    "\n",
    "**Nota:** Realmente tengo preparado otro con ejemplo, pero haciendo pruebas con el me ha dado peores resultados, por lo que he decido utilizar zero-shot learning mejor que one-shot o few-shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template base para la tarea de identificacion de eventos relevantes\n",
    "base_template_path = Path(f\"./prompts/templates/{business_developments_task}/base_template.txt\")\n",
    "\n",
    "# Template base con un ejemplo (no funciona tan bien)\n",
    "# base_template_path = Path(f\"./prompts/templates/{business_developments_task}/base_template_with_example.txt\")\n",
    "\n",
    "# Leemos el archivo del template\n",
    "with open(base_template_path, 'r') as prompt_file:\n",
    "    base_template = prompt_file.read()\n",
    "\n",
    "prompt_text = base_template\n",
    "\n",
    "n_prompt_tokens = count_tokens(encoding, prompt_text)\n",
    "\n",
    "print(n_prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparar chunks de texto a analizar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya hemos definido previamente la función `generate_chunks()`. En este caso, solo tenemos que utilizarla correctamente:\n",
    "\n",
    "```python\n",
    "max_tokens_per_chunk = MODEL_CONTEXT_LENGTH - MAX_GENERATION_LENGTH - EXTRA_TOKENS_FOR_TOKENIZATION_VARIABILITY - n_prompt_tokens\n",
    "max_tokens_per_chunk = 4097 - 1000 - 50 - 220\n",
    "max_tokens_per_chunk = 2827\n",
    "```\n",
    "\n",
    "**Nota:** En este caso me encontré un error algo extraño de OpenAI (`Error in OpenAICallbackHandler.on_llm_end callback: TypeError(\"unsupported operand type(s) for /: 'NoneType' and 'int'\")`), asi que modifique la longitud de los chunks y funciona..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En este caso me encontré un error algo extraño de OpenAI: \n",
    "# `Error in OpenAICallbackHandler.on_llm_end callback: TypeError(\"unsupported operand type(s) for /: 'NoneType' and 'int'\")` \n",
    "# asi que modifique la longitud de los chunks y funciona...\n",
    "max_tokens_per_chunk = MODEL_CONTEXT_LENGTH - MAX_GENERATION_LENGTH - EXTRA_TOKENS_FOR_TOKENIZATION_VARIABILITY - n_prompt_tokens - 50\n",
    "\n",
    "chunks = generate_chunks(df, max_tokens_per_chunk)\n",
    "\n",
    "total_token_count = 0\n",
    "for i in range(0, len(chunks)):\n",
    "    token_count = count_tokens(encoding, chunks[i])\n",
    "    print(f\"{i}: {token_count}\")\n",
    "    total_token_count += token_count\n",
    "\n",
    "print(f\"Total: {total_token_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - Correr el prompt por el LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el prompt template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"input_text\"],\n",
    "    template=prompt_text + \"\\n{input_text}\",\n",
    ")\n",
    "\n",
    "# Crear la cadena LLMChain\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Correr los prompts generados anteriormente por el LLM\n",
    "bd_outputs = []\n",
    "for i in range(len(chunks)):\n",
    "    print(f\"Chunk {i}\")\n",
    "    output = llm_run(llm_chain, chunks[i])\n",
    "    bd_outputs.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 - Formatear la salida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividimos aquellas lineas de texto que se encuentran en formato `\"Evento\" | -10 a 10 | \"Razón del score\"` y las almacenamos en un DataFrame de Pandas. Por supuesto, aquellas que no cumplen con las reglas son ignoradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_business_development_text(text):\n",
    "    parts = text.split('|')\n",
    "    if len(parts) == 3:\n",
    "        business_development_summary = parts[0].strip()\n",
    "        # score = int(parts[1].strip())\n",
    "        score = parts[1].strip()\n",
    "        reason_for_score = parts[2].strip()\n",
    "        return business_development_summary, score, reason_for_score\n",
    "    else:\n",
    "        print(f\"Invalid format for: {text}\")\n",
    "        return None, None, None\n",
    "\n",
    "bd_output_dfs = []\n",
    "for output in bd_outputs:\n",
    "    bd_output_dfs.append(pd.DataFrame({\"RAW Output\": output.split(\"\\n\")}))\n",
    "\n",
    "bd_output_df = pd.concat(bd_output_dfs)\n",
    "bd_output_df = bd_output_df[bd_output_df[\"RAW Output\"] != \"\"]\n",
    "bd_output_df = bd_output_df.drop_duplicates([\"RAW Output\"]).reset_index(drop=True).copy()\n",
    "\n",
    "# Creamos nuevas columnas\n",
    "bd_output_df[['Business development', 'Score', 'Explanation']] = bd_output_df['RAW Output'].apply(lambda x: pd.Series(split_business_development_text(x)))\n",
    "bd_output_df = bd_output_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd_output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 - Comentarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variabilidad en el formato de los salidas\n",
    "\n",
    "Es evidente que, incluso aun estableciendo la temperatura del modelo a 0, se generan respuestas en diversos formatos. Por ejemplo:\n",
    "- Algunas respuestas comienza con listas numeradas\n",
    "- Las explicaciones de los scores a veces se presentan directamente y otras empizan como `\"This is a positive development...\"`\n",
    "\n",
    "Para mitigar esta variabilidad y lograr un formato más consistente, tenemos varios opciones:\n",
    "- Mejorar los prompts\n",
    "- Hacer fine-tuning del modelo\n",
    "\n",
    "En mi opinión, lo que daría mejores resultados es hacer un adecuado fine-tuning del modelo con ejemplos preparados en el formato deseado.\n",
    "\n",
    "Alternativamente, podríamos considerar volver a ejecutar secciones de texto que no se han formateado según lo previsto. Podríamos usar una configuración de temperatura más alta durante estas ejecuciones y verificar si la salida se alinea con nuestras expectativas.\n",
    "\n",
    "**Salidas Innecesarias (Desviación de las reglas establecidas en el prompt):**\n",
    "Como se evidencia, el modelo ocasionalmente genera texto que no sigue las instrucciones que proporcionamos. Afortunadamente, ignoramos estas salidas innecesarias durante el proceso de formateo. Para abordar este problema, deberíamos investigar por qué ocurre esto y explorar mejoras potenciales para evitar tales desviaciones de nuestras pautas en el futuro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Obtener información mediante preguntas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta tarea tiene como objetivo poner en práctica lo que hemos visto en la parte de Retrieval Augmented Generation (RAG). Especificamente, es una tarea de pregunta-respuesta con contexto, donde el contexto se almacena en una base de datos vectorial.\n",
    "\n",
    "Como elementos relevantes de la misma, utilizaremos:\n",
    "* Base de datos: [Chroma](https://www.trychroma.com/) (en memoria RAM)\n",
    "* Modelo de embedding: [OpenAI ADA-02](https://platform.openai.com/docs/guides/embeddings)\n",
    "* Modelo de lenguaje: [OpenAI GPT-3](https://platform.openai.com/docs/models) (`davinci-003`)\n",
    "\n",
    "En este caso vamos a preparar una aplicación muy simple, por lo que no vamos a introducir memoria, multi-query u otras de las mejoras que hemos discutido en la seccion de RAG avanzado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Modelo de embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "\n",
    "# Lee la clave de API desde el archivo de configuración\n",
    "with open('config.txt') as f:\n",
    "    config = dict(line.strip().split('=') for line in f)\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = \"https://gpt3tests.openai.azure.com/\"\n",
    "openai.api_version = \"2022-12-01\"\n",
    "openai.api_key = config.get(\"OPENAI_API_KEY\", \"\")\n",
    "\n",
    "# Nombre del despliegue en mi Azure OpenAI Studio is \"TextEmbeddingAda002\", el modelo es \"text-embedding-ada-002\"\n",
    "engine = \"TextEmbeddingAda002\"\n",
    "openai_api_version = \"2023-12-04\" \n",
    "\n",
    "openai_embeddings_model = AzureOpenAIEmbeddings(azure_endpoint=openai.api_base, azure_deployment=engine, openai_api_key=config.get(\"OPENAI_API_KEY\", \"\"), openai_api_version=openai.api_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Base de datos vectorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Ya tenemos el texto dividido en chunks (párrafos en nuestro caso)\n",
    "chunks = df[\"Paragraph\"].tolist()\n",
    "\n",
    "# Creamos documentos a partir de los chunks de texto\n",
    "documents = []\n",
    "for chunk in chunks:\n",
    "    doc =  Document(page_content=chunk, metadata={\"source\": \"local\"})\n",
    "    documents.append(doc)\n",
    "\n",
    "# Guardamos los chunks en una instancia de Chroma que reside en memoria RAM\n",
    "db = Chroma.from_documents(documents, openai_embeddings_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 - Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora escribamos la lógica real de la aplicación. Queremos crear una aplicación sencilla que \n",
    "1. Tome una pregunta del usuario\n",
    "2. Busque documentos relevantes para esa pregunta\n",
    "3. Pase los documentos recuperados y la pregunta inicial a un LLM\n",
    "4. Devuelva una respuesta\n",
    "\n",
    "Primero, necesitamos definir nuestra lógica para buscar en los documentos. LangChain define una interfaz `Retriever` que envuelve un índice que puede devolver Documentos relevantes dado una uery. Su funcionamiento es similar al de `db.similarity_search()`.\n",
    "\n",
    "El tipo más común de `Retriever` es el `VectorStoreRetriever`, que utiliza las capacidades de búsqueda de similitud de un vector store para facilitar la recuperación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el retriever, que devuelve los k textos mas similares a la pregunta\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corremos una pregunta de ejemplo\n",
    "retrieved_docs = retriever.invoke(\"Who are the Chief executives in Growth Point Properties?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los documentos recuperados\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece que recupera correctamente la información, la cual se encuentra dividida en 2 párrafos (si bien en el texto original realmente es uno) por los problemas del PDF que hemos comentado anteriormente (no se cargan correctamente los saltos de linea)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 - Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez tenemos los documentos que otorgarán de contexto al LLM, debemos pasarle dicha información dentro de un prompt (junto a la pregunta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from langchain.llms import AzureOpenAI\n",
    "\n",
    "# Lee la clave de API desde el archivo de configuración\n",
    "with open('config.txt') as f:\n",
    "    config = dict(line.strip().split('=') for line in f)\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = \"https://gpt3tests.openai.azure.com/\"\n",
    "openai.api_version = \"2022-12-01\"\n",
    "openai.api_key = config.get(\"OPENAI_API_KEY\", \"\")\n",
    "\n",
    "# Nombre del despliegue en mi Azure OpenAI Studio is \"TextEmbeddingAda002\", el modelo es \"text-embedding-ada-002\"\n",
    "engine = \"Davinci003\"\n",
    "model = \"text-davinci-003\"\n",
    "openai_api_version = \"2023-12-01\" \n",
    "\n",
    "TEMPERATURE = 0 # low temperature to avoid GPT's \"imagination\"\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    azure_endpoint=openai.api_base, \n",
    "    azure_deployment=engine, \n",
    "    openai_api_key=config.get(\"OPENAI_API_KEY\", \"\"), \n",
    "    openai_api_version=openai.api_version,\n",
    "    temperature=TEMPERATURE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plantilla especifica para Question-Answering\n",
    "custom_rag_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "custom_rag_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=custom_rag_template,\n",
    ")\n",
    "\n",
    "# Crear la cadena LLM simple\n",
    "qa_chain = LLMChain(llm=llm, prompt=custom_rag_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_run(chain, query):\n",
    "    start_time = time.time()  # Registrar el tiempo de inicio\n",
    "\n",
    "    with get_openai_callback() as cb:\n",
    "        result = chain.run(query)\n",
    "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
    "\n",
    "    end_time = time.time()  # Registrar el tiempo de finalización\n",
    "    execution_time = end_time - start_time  # Calcular el tiempo de ejecución en segundos\n",
    "    print(f'Time taken: {round(execution_time, 2)} seconds')\n",
    "\n",
    "    return result\n",
    "\n",
    "# El contexto no es mas que la concatenación de los documentos recuperados\n",
    "context = \"\"\n",
    "for i in range (0, len(retrieved_docs)):\n",
    "    doc = retrieved_docs[i]\n",
    "    context += f\"\\nDocument #{i}:\"\n",
    "    context += doc.page_content\n",
    "\n",
    "# La pregunta\n",
    "question = \"Who are the Chief executives in Growth Point Properties?\"\n",
    "\n",
    "# La query resultante que mandamos al modelo\n",
    "query = {\n",
    "    'context': context,\n",
    "    'question': question\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_run(qa_chain, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 - Comentarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos incrementar la complejidad de la aplicación añadiendo funcionalidades tales como:\n",
    "* [Streaming](https://python.langchain.com/docs/use_cases/question_answering/streaming). En vez de devolver la respuesta de golpe una vez ha sido generada, podemos ir devolviendola token a token según se genera (similar al comportamiento de Chat-GPT).\n",
    "  \n",
    "* [Memoria](https://python.langchain.com/docs/use_cases/question_answering/chat_history). Este sistema está pensado para interacciones puntuales sobre el modelo. No permite una conversación continuada. Para ello, tendriamos que implementar un historial del chat (i.e., una memoria de corto plazo).\n",
    "  \n",
    "* [Fuentes](https://python.langchain.com/docs/use_cases/question_answering/sources). Si bien para este ejemplo particular no tiene mucho sentido ya que solo hay un documento sobre el cual estamos haciendo preguntas. Podria darse el caso de que queremos extender la funcionalidad de la aplicacion para analizar un documento de una compañia, sino múltiples documentos, en tal caso, estaria bien que el modelo nos indicase cual es el documento (y que parrafos del mismo) ha utilizado para generar su respuesta.\n",
    "\n",
    "A su vez, **si los párrafos fueran muy largos, deberiamos hacer estimaciones del número de tokens para poder ajustar lo mejor posible las queries** (similar a las secciones 2 y 3 de este proyecto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siguiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente y última lección, comentaremos ciertos conceptos avanzados como son el despliegue de aplicaciones LangChain, la monitorización o el LangChain Expression Language (LCEL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
