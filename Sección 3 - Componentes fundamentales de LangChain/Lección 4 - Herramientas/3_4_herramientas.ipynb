{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Herramientas\n",
    "\n",
    "**Sumario**\n",
    "1. Clase `Tool`\n",
    "<br></br>\n",
    "2. Clase `StructuredTool`\n",
    "<br></br>\n",
    "3. Subclase de `BaseTool`\n",
    "   1. Único parámetro\n",
    "   2. Múltiples parámetros\n",
    "   3. Ejemplo: Agente con reconocimiento de imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from langchain.llms import AzureOpenAI\n",
    "\n",
    "# Lee la clave de API desde el archivo de configuración\n",
    "with open('config.txt') as f:\n",
    "    config = dict(line.strip().split('=') for line in f)\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = \"https://gpt3tests.openai.azure.com/\"\n",
    "openai.api_version = \"2022-12-01\"\n",
    "openai.api_key = config.get(\"OPENAI_API_KEY\", \"\")\n",
    "\n",
    "# Nombre del despliegue en mi Azure OpenAI Studio is \"Davinci003\", el modelo es \"text-davinci-003\"\n",
    "engine = \"Davinci003\"\n",
    "model = \"text-davinci-003\"\n",
    "openai_api_version = \"2023-12-01\" \n",
    "\n",
    "# Nombre del despliegue en mi Azure OpenAI Studio para el modelo de embeddings es \"TextEmbeddingAda002\"\n",
    "embeddings_engine = \"TextEmbeddingAda002\"\n",
    "\n",
    "max_tokens = 1000\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    azure_endpoint=openai.api_base, \n",
    "    azure_deployment=engine, \n",
    "    openai_api_key=config.get(\"OPENAI_API_KEY\", \"\"), \n",
    "    openai_api_version=openai.api_version,\n",
    "    # temperature=0, # Podemos poner la temperatura a 0 si queremos reducir la variabilidad de las respuestas\n",
    ")\n",
    "llm.openai_api_base = openai.api_base \n",
    "llm.max_tokens = max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "def call_agent_with_translate(agent, es_eng_chain, eng_es_chain, query):\n",
    "\n",
    "    print(f\"Input: {query}\")\n",
    "\n",
    "    with get_openai_callback() as cb:\n",
    "        query_translated = es_eng_chain(query)[\"text\"]\n",
    "        query_translated = query_translated.replace('\\n', '') # Simplemente para mostrarlo mejor en este ejemplo\n",
    "        es_eng_token_count = cb.total_tokens\n",
    "    print(f\"Input (traducido al inglés): {query_translated}\")\n",
    "\n",
    "    with get_openai_callback() as cb:\n",
    "        result = agent(query_translated)\n",
    "        agent_token_count = cb.total_tokens\n",
    "    print(f\"Output: {result['output']}\")\n",
    "\n",
    "    with get_openai_callback() as cb:\n",
    "        output_translated = eng_es_chain(result[\"output\"])[\"text\"]\n",
    "        output_translated = output_translated.replace('\\n', '') # Simplemente para mostrarlo mejor en este ejemplo\n",
    "        eng_es_token_count = cb.total_tokens\n",
    "    print(f\"Output (traducido al español): {output_translated}\")\n",
    "\n",
    "    total_tokens = es_eng_token_count + agent_token_count + eng_es_token_count\n",
    "    print(f'He usado un total de {total_tokens} tokens')\n",
    "\n",
    "    return result\n",
    "\n",
    "# Chain para traducir el texto de entrada de español a inglés\n",
    "translate_eng_es_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate the following spanish text to english (write only the translated text): {input}\"\n",
    ")\n",
    "translate_eng_es_chain = LLMChain(llm=llm, prompt=translate_eng_es_prompt)\n",
    "# Chain para traducir el texto de salida del modelo de inglés a español\n",
    "translate_es_eng_prompt = ChatPromptTemplate.from_template(\n",
    "    # \"Traduce el siguiente texto a español, deja las operaciones matematicas sin modificar: {input}\"\n",
    "    \"Si el siguiente texto esta en inglés, traducelo al español, en caso contrario, simplemente devuelve el mismo texto: {input}\"\n",
    ")\n",
    "translate_es_eng_chain = LLMChain(llm=llm, prompt=translate_es_eng_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Clase `Tool`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El enfoque más simple para definir una herramienta en Langchain. La dataclass Tool encapsula funciones que aceptan un único string de entrada y devuelven un string de salida.\n",
    "\n",
    "`pip install wikipedia`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "\n",
    "# Función propia mediante la cual devolvemos el valor hash de un string de entrada\n",
    "def return_hash(input: str):\n",
    "    return str(hash(input))\n",
    "\n",
    "hash_tool = Tool.from_function(\n",
    "    func=return_hash,\n",
    "    name=\"Hash\",\n",
    "    description=\"Useful for when you need to estimate the hash of an input\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "\n",
    "wikipedia = WikipediaAPIWrapper(\n",
    "    doc_content_chars_max=2000\n",
    ")\n",
    "wikipedia_tool = Tool.from_function(\n",
    "    func=wikipedia.run,\n",
    "    name=\"Wikipedia\",\n",
    "    description=\"Wikipedia, useful for when you need to answer questions about history, science, biology, etc.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain import LLMMathChain\n",
    "\n",
    "llm_math_chain = LLMMathChain.from_llm(llm=llm, verbose=True)\n",
    "\n",
    "calculator_tool = Tool.from_function(\n",
    "    func=llm_math_chain.run,\n",
    "    name=\"Calculator\",\n",
    "    description=\"Useful for when you need to answer questions about math\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType, initialize_agent\n",
    "\n",
    "tools = [hash_tool, wikipedia_tool, calculator_tool]\n",
    "\n",
    "zero_shot_agent_executor = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "result = call_agent_with_translate(\n",
    "    zero_shot_agent_executor, \n",
    "    translate_eng_es_chain,\n",
    "    translate_es_eng_chain,\n",
    "    \"¿Cuanta gente vive en Paris?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = call_agent_with_translate(\n",
    "    zero_shot_agent_executor, \n",
    "    translate_eng_es_chain,\n",
    "    translate_es_eng_chain,\n",
    "    \"Cual es el resultado de multiplicar el hash de la palabra Everest por 0.005?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "everest_hash = return_hash('Everest')\n",
    "print(f\"Hash: {everest_hash}\")\n",
    "print(f\"Resultado: {float(everest_hash) * 0.005}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Clase `StructuredTool`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la clase `Tool` podemos usar nuestras propias funciones, pero estamos limitados a `str -> str`. Si queremos utilizar funciones con argumentos más estructurados, podemos usar la clase `StructuredTool`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import StructuredTool\n",
    "\n",
    "# Es importante que indiquemos el tipo de los parámetros ya que dan \n",
    "# información al agente\n",
    "def avg_2_nums(num1: float, num2: float) -> float:\n",
    "    return (num1 + num2) / 2\n",
    "\n",
    "avg_2_nums_tool = StructuredTool.from_function(\n",
    "    func=avg_2_nums,\n",
    "    name=\"Average_two_numbers\",\n",
    "    description=\"Useful for when you need to estimate the average of two numbers\",\n",
    ")\n",
    "\n",
    "# Las herramientas con multiples inputs sona dia de hoy solo compatiblse con el agente STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION\n",
    "structured_agent_executor = initialize_agent(\n",
    "    [avg_2_nums_tool],\n",
    "    llm,\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(structured_agent_executor.agent.llm_chain.prompt.messages[0].prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = call_agent_with_translate(\n",
    "    structured_agent_executor, \n",
    "    translate_eng_es_chain,\n",
    "    translate_es_eng_chain,\n",
    "    \"Cual es la media de 3.7 y 7.75?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a hacer un ejemplo un poco más complejo donde simulamos que enviamos un correo a una dirección y le vamos a poder al agente que por favor lo envie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "# Función donde simulamos el envio de un correo electronico cuyo éxito es del 50% \n",
    "# (para mostrar como se comporta el agente en cada caso)\n",
    "def send_email(direccion: str, cabecero: str, mensaje: str) -> str:\n",
    "    print(f\"Enviando email a la dirección: {direccion}\")\n",
    "\n",
    "    if random.random() < 0.5:\n",
    "\n",
    "        for i in range(3, 0, -1):\n",
    "            print(f\"Enviando, por favor espere... {i}\")\n",
    "            time.sleep(1)\n",
    "\n",
    "        print(\"Enviado correctamente\")\n",
    "        print(f\"Cabecero: {cabecero}\")\n",
    "        print(f\"Mensaje: {mensaje}\")\n",
    "\n",
    "        return \"Success\"\n",
    "    else:\n",
    "        return \"Error\"\n",
    "\n",
    "send_email(\"test@gmail.com\", \"Prueba\", \"Hola Mundo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "send_email_tool = StructuredTool.from_function(\n",
    "    func=send_email,\n",
    "    name=\"send_email\",\n",
    "    description=\"Useful for when you need to send an email to a specific adress\",\n",
    ")\n",
    "\n",
    "structured_agent_executor = initialize_agent(\n",
    "    [send_email_tool],\n",
    "    llm,\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = call_agent_with_translate(\n",
    "    structured_agent_executor, \n",
    "    translate_eng_es_chain,\n",
    "    translate_es_eng_chain,\n",
    "    \"Envia un correo a test@gmail.com con el mensaje 'Hola, que tal?' y un cabecero del mail que consideres apropiado\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Subclase de `BaseTool`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra opción es subclasificar directamente `BaseTool`. Esto es útil si queremos tener más control sobre las variables o si queremos propagar callbacks de llamada a cadenas anidadas u otras herramientas. \n",
    "\n",
    "Este enfoque se puede utilizar para definir herramientas que requieren uno o más parámetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Único parámetro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a empezar con una herramienta simple donde estimamos la circunferencia de un círculo a partir de su radio.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_3_4/circunferencia.png\" width=\"300\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "La idea seria utilizar esta herramienta para responder preguntas como la siguiente:\n",
    "\n",
    "```\n",
    "Puedes decirme cual es la circunferencia de un círculo con radio 7.81mm?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import BaseTool\n",
    "from math import pi\n",
    "from typing import Callable, Optional,  Union\n",
    "\n",
    "class CircumferenceTool(BaseTool):\n",
    "    name = \"Circumference calculator\"\n",
    "    description = \"use this tool when you need to calculate a circumference using the radius of a circle\"\n",
    "\n",
    "    def _run(self, radius: Union[int, float]):\n",
    "        return float(radius)*2.0*pi\n",
    "\n",
    "    def _arun(self, radius: int):\n",
    "        raise NotImplementedError(\"This tool does not support async\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que vimos más arriba, LangChain requiere dos atributos para reconocer un objeto como una herramienta válida. Estos son los parámetros `name` y `description`. \n",
    "\n",
    "A continuación, tenemos dos métodos: `_run()` y `_arun()`. Cuando se usa una herramienta, se llama al método `_run()` de forma predeterminada. Se llama al método `_arun()` cuando se debe usar una herramienta de forma asincrónica.\n",
    "\n",
    "A partir de aquí, creamos un agente conversacional simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "circumference_conversational_memory = ConversationBufferWindowMemory(\n",
    "    memory_key='chat_history',\n",
    "    k=5, # Recuerda las 5 últimas interacciones humanas\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# initialize agent with tools\n",
    "circumference_agent_executor = initialize_agent(\n",
    "    agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "    tools=[CircumferenceTool()],\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    max_iterations=10,\n",
    "    # early_stopping_method: Si se para el modelo antes de tiempo (por ejemplo si se ha metico en un bucle), \n",
    "    # el output resultante considera todo lo que ha hecho hasta el momento para dar una mejor respuesta\n",
    "    early_stopping_method='generate', \n",
    "    memory=circumference_conversational_memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = call_agent_with_translate(\n",
    "    circumference_agent_executor, \n",
    "    translate_eng_es_chain,\n",
    "    translate_es_eng_chain,\n",
    "    \"Puedes decirme cual es la circunferencia de un círculo con radio 7.81mm?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si nos encontrarnos con un `ValueError` porque el LLM decide pasar directamente \"7.81mm\" a la calculadora en lugar de \"7.81\".\n",
    "\n",
    "Cuando una herramienta encuentra un error y la excepción no se captura, el agente dejará de ejecutarse. Si desea que el agente continúe su ejecución, puede lanzar un `ToolException` y configurar `handle_tool_error` en consecuencia.\n",
    "\n",
    "Cuando se lanza un `ToolException`, el agente no deja de funcionar. En cambio, manejará la excepción de acuerdo con la variable `handle_tool_error` de la herramienta, y **el resultado del procesamiento se devolverá al agente como observación y se imprimirá en rojo**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.base import ToolException\n",
    "\n",
    "def handle_error(error: ToolException) -> str:\n",
    "    return f\"The following errors occurred during tool execution:{error.args[0]}\"\n",
    "\n",
    "class SafeCircumferenceTool(BaseTool):\n",
    "    name: str = \"Circumference calculator with exception handling\"\n",
    "    description: str = \"use this tool when you need to calculate a circumference using the radius of a circle.\"\n",
    "    handle_tool_error: Optional[\n",
    "        Union[bool, str, Callable[[ToolException], str]]\n",
    "    ] = handle_error\n",
    "\n",
    "    def _run(self, radius: Union[int, float]):\n",
    "        try:\n",
    "            return float(radius)*2.0*pi\n",
    "        except ValueError as error:\n",
    "            raise ToolException(error.args[0])\n",
    "\n",
    "    def _arun(self, radius: int):\n",
    "        raise NotImplementedError(\"This tool does not support async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_circumference_conversational_memory = ConversationBufferWindowMemory(\n",
    "    memory_key='chat_history',\n",
    "    k=5, # Recuerda las 5 últimas interacciones humanas\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# initialize agent with tools\n",
    "safe_circumference_agent_executor = initialize_agent(\n",
    "    agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "    tools=[SafeCircumferenceTool()],\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    max_iterations=10,\n",
    "    # early_stopping_method: Si se para el modelo antes de tiempo (por ejemplo si se ha metico en un bucle), \n",
    "    # el output resultante considera todo lo que ha hecho hasta el momento para dar una mejor respuesta\n",
    "    early_stopping_method='generate', \n",
    "    memory=safe_circumference_conversational_memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = call_agent_with_translate(\n",
    "    safe_circumference_agent_executor, \n",
    "    translate_eng_es_chain,\n",
    "    translate_es_eng_chain,\n",
    "    \"Puedes decirme cual es la circunferencia de un círculo con radio 7.81mm?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que el agente no se bloquea en este ejemplo, pero tampoco puede comprender correctamente qué debe hacer con el error. Podríamos abordar este problema desde varias perspectivas diferentes:\n",
    "\n",
    "* Reformular nuestra pregunta para que el agente pueda entenderla.\n",
    "<br></br>\n",
    "* Mejorar la herramienta para que preprocese automáticamente la entrada para evitar un ValueError. Por ejemplo, podemos poner un paso intermedio que traduzca el input que recibe el método en un valor numérico. Esto podria hacerse con un LLM.\n",
    "<br></br>\n",
    "* Generar prompts para el modelo para que pueda comprender cómo debe comportarse con un error de formato de este tipo. Por ejemplo, reintenandolo mediante una modificación del input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Múltiples parámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la *calculadora de circunferencias*, solo podíamos ingresar un único valor (es decir, el radio). Sin embargo, a menudo necesitaremos múltiples parámetros.\n",
    "\n",
    "Para demostrar cómo hacerlo, construiremos una *calculadora de hipotenusas*. La herramienta nos ayudará a calcular la hipotenusa de un triángulo dada una combinación de longitudes de lados y/o ángulos del triángulo.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_3_4/hipotenusa.png\" width=\"300\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Queremos múltiples entradas aquí porque calculamos la hipotenusa del triángulo con diferentes valores (los lados y el ángulo). Además, no necesitamos todos los valores. **Podemos calcular la hipotenusa con cualquier combinación de dos o más parámetros**.\n",
    "\n",
    "Ejemplo de preguntas:\n",
    "\n",
    "```\n",
    "Si tengo un triángulo con dos lados de 10 cm y 6 cm de longitud, ¿cuál es la longitud de la hipotenusa?\n",
    "\n",
    "Si tengo un triángulo cuyo 'lado adyacente' mide 10 cm de longitud y su ángulo es de 30.96 grados, ¿cuál es la longitud de la hipotenusa?\n",
    "\n",
    "Si tengo un triángulo cuyo 'lado opuesto' mide 6 cm de longitud y su ángulo es de 30.96 grados, ¿cuál es la longitud de la hipotenusa?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def angulos_triangulo_dos_lados(a, b):\n",
    "    # Calcula la hipotenusa del triángulo rectángulo\n",
    "    c = math.sqrt(a**2 + b**2)\n",
    "\n",
    "    # Calcula los ángulos usando funciones trigonométricas\n",
    "    angulo_alfa = math.degrees(math.atan2(b, a))\n",
    "    angulo_rect = 90.0  # Dado que es un triángulo rectángulo\n",
    "    angulo_theta = 180.0 - angulo_alfa - angulo_rect  # La suma de los ángulos en un triángulo es 180 grados\n",
    "    \n",
    "    angulo_adyacente = angulo_theta\n",
    "    angulo_opuesto = angulo_alfa\n",
    "\n",
    "    return angulo_adyacente, angulo_opuesto\n",
    "\n",
    "a = 10 # lado adyacente\n",
    "b = 6 # lado opuesto \n",
    "\n",
    "angulos = angulos_triangulo_dos_lados(a, b)\n",
    "print(f\"Ángulo adyacente (theta en nuestro diagrama): {angulos[0]:.2f} grados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from math import sqrt, cos, sin, radians\n",
    "\n",
    "desc = (\n",
    "    \"use this tool when you need to calculate the length of a hypotenuse\"\n",
    "    \"given one or two sides of a triangle and/or an angle (in degrees). \"\n",
    "    \"To use the tool, you must provide at least two of the following parameters \"\n",
    "    \"['adjacent_side', 'opposite_side', 'angle']. Parameters are float numbers.\"\n",
    ")\n",
    "\n",
    "class HypotenuseTool(BaseTool):\n",
    "    name = \"hypotenuse_calculator\"\n",
    "    description = desc\n",
    "    \n",
    "    def _run(\n",
    "        self,\n",
    "        adjacent_side: Optional[Union[int, float]] = None,\n",
    "        opposite_side: Optional[Union[int, float]] = None,\n",
    "        angle: Optional[Union[int, float]] = None\n",
    "    ):\n",
    "        # check for the values we have been given\n",
    "        if adjacent_side and opposite_side:\n",
    "            return sqrt(float(adjacent_side)**2 + float(opposite_side)**2)\n",
    "        elif adjacent_side and angle:\n",
    "            return adjacent_side / cos(radians(float(angle)))\n",
    "        elif opposite_side and angle:\n",
    "            return opposite_side / sin(radians(float(angle)))\n",
    "        else:\n",
    "            return \"Could not calculate the hypotenuse of the triangle. Need two or more of `adjacent_side`, `opposite_side`, or `angle`.\"\n",
    "    \n",
    "    def _arun(self, query: str):\n",
    "        raise NotImplementedError(\"This tool does not support async\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "**Nota:** Al igual que con `StructuredTool.from_function()`, las herramientas con multiples inputs sona dia de hoy solo compatiblse con el agente `STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION`\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotenuse_conversational_memory = ConversationBufferWindowMemory(\n",
    "    memory_key='chat_history',\n",
    "    k=5, # Remember 5 last human interactions\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "hypotenuse_agent_executor = initialize_agent(\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    tools=[HypotenuseTool()],\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    max_iterations=3,\n",
    "    early_stopping_method='generate',\n",
    "    memory=hypotenuse_conversational_memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = call_agent_with_translate(\n",
    "    hypotenuse_agent_executor, \n",
    "    translate_eng_es_chain,\n",
    "    translate_es_eng_chain,\n",
    "    \"Si tengo un triángulo con dos lados de 10 cm y 6 cm de longitud, ¿cuál es la longitud de la hipotenusa?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = call_agent_with_translate(\n",
    "    hypotenuse_agent_executor, \n",
    "    translate_eng_es_chain,\n",
    "    translate_es_eng_chain,\n",
    "    \"Si tengo un triángulo cuyo 'lado adyacente' mide 10 cm de longitud y su ángulo es de 30.96 grados, ¿cuál es la longitud de la hipotenusa?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = call_agent_with_translate(\n",
    "    hypotenuse_agent_executor, \n",
    "    translate_eng_es_chain,\n",
    "    translate_es_eng_chain,\n",
    "    \"Si tengo un triángulo cuyo 'lado opuesto' mide 6 cm de longitud y su ángulo es de 30.96 grados, ¿cuál es la longitud de la hipotenusa?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Ejemplo: Agente de reconocimiento de imágenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si bien ya hemos visto dos ejemplos de cómo crear herramientas personalizadas con `BaseTool`, lo cierto es que su utilidad era algo limitada. Por ello, vamos mostrar un caso de uso \"más potente\".\n",
    "\n",
    "Inspirándonos en el artículo [\"HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face\"](https://arxiv.org/abs/2303.17580), tomaremos un modelo \"experto\" que ha sido entrenado en una tarea específica que nuestro LLM no puede realizar. En este caso vamos a usar [BLIP](https://arxiv.org/abs/2201.12086), un modelo desarrollado por Salesfore, el cual toma una imagen y la describe. [Este modelo se encuentra alojado de forma libre en HuggingFace](https://huggingface.co/Salesforce/blip-image-captioning-large).\n",
    "\n",
    "`pip install transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "\n",
    "# specify model to be used\n",
    "hf_model = \"Salesforce/blip-image-captioning-base\"\n",
    "# use GPU if it's available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# preprocessor will prepare images for the model\n",
    "processor = BlipProcessor.from_pretrained(hf_model)\n",
    "# then we initialize the model itself\n",
    "model = BlipForConditionalGeneration.from_pretrained(hf_model).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El proceso que seguiremos aquí es el siguiente:\n",
    "\n",
    "* Descargamos una imagen\n",
    "* La abrimos como un objeto PIL en Python\n",
    "* Cambia el tamaño y normaliza la imagen utilizando el \"procesador\" asociado a BLIP\n",
    "* Pasamos la imagen procesada por el modelo BLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.request\n",
    "from PIL import Image\n",
    "\n",
    "# Descargamos la imagen\n",
    "img_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Brillenkaiman_Caiman_yacare.jpg/1200px-Brillenkaiman_Caiman_yacare.jpg\"\n",
    "urllib.request.urlretrieve(img_url, \"img.jpg\")\n",
    "\n",
    "# Cargamos la imagen\n",
    "image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos que es lo que dice nuestro modelo al \"ver\" la imagen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesamos la imagen\n",
    "inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# La pasamos por el modelo BLIP\n",
    "out = model.generate(**inputs, max_new_tokens=20)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto confirmamos que el proceso funciona correctamente. El siguiente paso es destilar los pasos que hemos seguido en una herramienta que el agente pueda utilizar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptionTool(BaseTool):\n",
    "    name = \"image_captioner\"\n",
    "    description = (\n",
    "        \"use this tool when given the URL of an image that you'd like to be \"\n",
    "        \"described. It will return a simple caption describing the image.\"\n",
    "    )\n",
    "    \n",
    "    def _run(self, url: str):\n",
    "        # Descargamla imagen y la convierte en un objeto PIL\n",
    "        urllib.request.urlretrieve(url, \"img.jpg\")\n",
    "        image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\n",
    "        # Preprocesa la imagen\n",
    "        inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "        # Pasa el input por el modelo y general el caption\n",
    "        out = model.generate(**inputs, max_new_tokens=20)\n",
    "        caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "        return caption\n",
    "    \n",
    "    def _arun(self, query: str):\n",
    "        raise NotImplementedError(\"This tool does not support async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blip_conversational_memory = ConversationBufferWindowMemory(\n",
    "    memory_key='chat_history',\n",
    "    k=5, # Remember 5 last human interactions\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "blip_agent_executor = initialize_agent(\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    tools=[ImageCaptionTool()],\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    max_iterations=10,\n",
    "    early_stopping_method='generate',\n",
    "    memory=blip_conversational_memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blip_agent_executor(f\"What does this image show?\\n{img_url}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
