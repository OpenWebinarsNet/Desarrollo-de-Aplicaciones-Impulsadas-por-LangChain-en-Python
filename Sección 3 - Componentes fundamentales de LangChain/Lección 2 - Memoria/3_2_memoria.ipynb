{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memoria\n",
    "\n",
    "**Sumario**\n",
    "\n",
    "1. Memoria conversacional de búfer\n",
    "   1. `ConversationBufferMemory`\n",
    "   2. `ConversationBufferWindowMemory`\n",
    "   3. `ConversationTokenBufferMemory`\n",
    "   4. `ConversationSummaryMemory`\n",
    "<br></br>\n",
    "1. Memoria conversacional con entidades\n",
    "   1. `ConversationEntityMemory`\n",
    "<br></br>\n",
    "1. Memoria conversacional con un grafo de conocimiento\n",
    "   1. `ConversationKGMemory`\n",
    "<br></br>\n",
    "1. Memoria basada en bases de datos vectoriales\n",
    "   1. `VectorStoreRetrieverMemory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from langchain.llms import AzureOpenAI\n",
    "\n",
    "# Lee la clave de API desde el archivo de configuración\n",
    "with open('config.txt') as f:\n",
    "    config = dict(line.strip().split('=') for line in f)\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = \"https://gpt3tests.openai.azure.com/\"\n",
    "openai.api_version = \"2022-12-01\"\n",
    "openai.api_key = config.get(\"OPENAI_API_KEY\", \"\")\n",
    "\n",
    "# Nombre del despliegue en mi Azure OpenAI Studio is \"Davinci003\", el modelo es \"text-davinci-003\"\n",
    "engine = \"Davinci003\"\n",
    "model = \"text-davinci-003\"\n",
    "openai_api_version = \"2023-12-01\" \n",
    "\n",
    "# Nombre del despliegue en mi Azure OpenAI Studio para el modelo de embeddings es \"TextEmbeddingAda002\"\n",
    "embeddings_engine = \"TextEmbeddingAda002\"\n",
    "\n",
    "max_tokens = 1000\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    azure_endpoint=openai.api_base, \n",
    "    azure_deployment=engine, \n",
    "    openai_api_key=config.get(\"OPENAI_API_KEY\", \"\"), \n",
    "    openai_api_version=openai.api_version,\n",
    "    # temperature=0, # Podemos poner la temperatura a 0 si queremos reducir la variabilidad de las respuestas\n",
    ")\n",
    "llm.openai_api_base = openai.api_base \n",
    "llm.max_tokens = max_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Memoria conversacional de búfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La memoria de búfer es la forma más simple de memoria. Simplemente implica mantener un búfer de todos los mensajes anteriores. Hay diferentes tipos:\n",
    "\n",
    "* `ConversationBufferMemory`. \n",
    "* `ConversationBufferWindowMemory`. \n",
    "* `ConversationTokenBufferMemory`. \n",
    "* `ConversationSummaryMemory`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - `ConversationBufferMemory`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Permite almacenar mensajes y luego extraer los mensajes en una variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Lo siguiente es una conversación amistosa entre un humano y una inteligencia artificial. La IA es conversadora y proporciona muchos detalles específicos de su contexto. Si la IA no conoce la respuesta a una pregunta, dice honestamente que no lo sabe.\n",
    "\n",
    "Conversación actual:\n",
    "{history}\n",
    "Humano: {input}\n",
    "IA:\n",
    "\"\"\"\n",
    "\n",
    "chat_prompt = PromptTemplate.from_template(template)\n",
    "buffer_memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    prompt=chat_prompt,\n",
    "    memory=buffer_memory,\n",
    "    verbose=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actualicemos el prompt de entrada al español"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de empezar, si nos fijamos, la cadena tiene una prompt en inglés que si bien no deberia impedir que el modelo funcionase correctamente porque la tarea es muy simple, es conveniente que la actualicemos, ya que vamos a conversar en español."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.prompt = chat_prompt\n",
    "\n",
    "conversation.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Hola, me llamo Fernando\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"¿Cuanto es 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"¿Cómo me llamo?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Como acceder al búfer de memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_memory.save_context({\"input\": \"¿Cual es el modelo que te soporta?\"}, \n",
    "                    {\"output\": \"Mi modelo es GPT-3\"})\n",
    "\n",
    "print(buffer_memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"¿Cual decias que era tu modelo?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - `ConversationBufferWindowMemory`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mantiene una lista de las interacciones de la conversación a lo largo del tiempo. Solo utiliza las últimas $K$ interacciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "buffer_window_memory = ConversationBufferWindowMemory(k=1) # Solo va a recordar 1 interacción que hayamos hecho\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    prompt=chat_prompt,\n",
    "    memory=buffer_window_memory,\n",
    "    verbose=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Hola, me llamo Fernando\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"¿Cuanto es 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"¿Cómo me llamo?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 - `ConversationTokenBufferMemory`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mantiene un búfer de interacciones recientes en la memoria y utiliza la longitud de tokens en lugar del número de interacciones para determinar cuándo \"cortar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "\n",
    "buffer_token_memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=20) # Guarda los últimos X tokens\n",
    "buffer_token_memory.save_context({\"input\": \"¿Cómo se llama el planeta en el que vivimos\"},\n",
    "                    {\"output\": \"La Tierra\"})\n",
    "buffer_token_memory.save_context({\"input\": \"¿En qué galaxia se situa?\"},\n",
    "                    {\"output\": \"La Via Láctea\"})\n",
    "buffer_token_memory.save_context({\"input\": \"¿En qué sistema?\"}, \n",
    "                    {\"output\": \"En el Sistema Solar\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver como cambia el contexto segun cambiamos el número máximo de tokens permitidos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_token_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 - `ConversationSummaryMemory`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En lugar de limitar el número de intercambios o tokens, podemos escribir un resumen de la conversación hasta ahora y lo usamos como memoria. \n",
    "\n",
    "Este enfoque mantiene la última interacción en la memoria y resume las anteriores. Así que, cuando llega una nueva interacción, la anterior se combina con el resumen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "agenda = \"\"\"\n",
    "- Hay una reunion a las 10 am con el equipo de ingenieria, asi que necesitas tener preparada la presentación PowerPoint.\n",
    "- De 11 am a 13 pm hay que trabajar en el proyecto de Python, lo cual será rápido porque el proyecto esta casi terminado.\n",
    "- Al mediodía, almuerzo en el restaurante italiano con un cliente ha conducido más de una hora para poder reunirse contigo. Asegurate de llevar el portátil para mostrar la última demo del modelo LLM que has preparado\n",
    "\"\"\"\n",
    "\n",
    "buffer_summary_memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "buffer_summary_memory.save_context({\"input\": \"Hola\"}, {\"output\": \"¿Qué tal?\"})\n",
    "buffer_summary_memory.save_context({\"input\": \"No mucho, solo pasando el rato\"},\n",
    "                     {\"output\": \"Genial\"})\n",
    "buffer_summary_memory.save_context({\"input\": \"¿Qué hay en la agenda hoy?\"}, \n",
    "                     {\"output\": f\"{agenda}\"})\n",
    "\n",
    "buffer_summary_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langchain tiene varios prompts por defecto escritos en inglés que pueden modificar la memoria o la salida del modelo. Por ello, en casos como este, podemos actualizar el prompt para que se adecue más a nuestras necesidades:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory.prompt import SUMMARY_PROMPT\n",
    "\n",
    "SUMMARY_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_template = \"\"\"\n",
    "Resume progresivamente las líneas de conversación proporcionadas, añadiendo al resumen anterior para devolver un nuevo resumen.\n",
    "\n",
    "EJEMPLO\n",
    "Resumen actual:\n",
    "El humano pregunta qué piensa la IA sobre la inteligencia artificial. La IA piensa que la inteligencia artificial es una fuerza para el bien.\n",
    "\n",
    "Nuevas líneas de conversación:\n",
    "Humano: ¿Por qué piensas que la inteligencia artificial es una fuerza para el bien?\n",
    "IA: Porque la inteligencia artificial ayudará a los humanos a alcanzar su máximo potencial.\n",
    "\n",
    "Nuevo resumen:\n",
    "El humano pregunta qué piensa la IA sobre la inteligencia artificial. La IA piensa que la inteligencia artificial es una fuerza para \n",
    "el bien porque ayudará a los humanos a alcanzar su máximo potencial.\n",
    "FIN DEL EJEMPLO\n",
    "\n",
    "Resumen actual:\n",
    "{summary}\n",
    "\n",
    "Nuevas líneas de conversación:\n",
    "{new_lines}\n",
    "\n",
    "Nuevo resumen:\n",
    "\"\"\"\n",
    "summary_prompt = PromptTemplate.from_template(summary_template)\n",
    "\n",
    "buffer_summary_memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm, \n",
    "    max_token_limit=100,\n",
    "    prompt=summary_prompt,\n",
    ")\n",
    "buffer_summary_memory.save_context({\"input\": \"Hola\"}, {\"output\": \"¿Qué tal?\"})\n",
    "buffer_summary_memory.save_context({\"input\": \"No mucho, solo pasando el rato\"},\n",
    "                     {\"output\": \"Genial\"})\n",
    "buffer_summary_memory.save_context({\"input\": \"¿Qué hay en la agenda hoy?\"}, \n",
    "                     {\"output\": f\"{agenda}\"})\n",
    "\n",
    "buffer_summary_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=buffer_summary_memory,\n",
    "    verbose=True, \n",
    "    prompt=chat_prompt,\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"¿Tengo alguna reunion a las 10am?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"¿Cuantas diapositivas aconsejas para mi presentación?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_summary_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Memoria conversacional con entidades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - `ConversationEntityMemory`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta memoria extrae información sobre las entidades (utilizando LLMs) y construye su conocimiento sobre cada entidad según avanza la conversación\n",
    "\n",
    "Hemos preparado un ejemplo con 5 entidades (conocidas de antemano por el modelo y desconocidas tambié):\n",
    "* Jorge (desconocido)\n",
    "* Ignacio (desconocido)\n",
    "* Langchain (desconocido en 2021, es decir, datos del último modelo)\n",
    "* Nielsen (conocido)\n",
    "* Everest (conocido)\n",
    "\n",
    "Al generar el resumen de las entidades, podemos ver cómo el modelo incorpora información adicional a las entidades \"conocidas\" dada la información contextual proporcionada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory.prompt import ENTITY_MEMORY_CONVERSATION_TEMPLATE\n",
    "\n",
    "# Mostramos el template de base (en inglés)\n",
    "print(ENTITY_MEMORY_CONVERSATION_TEMPLATE.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro primer paso como en casos anteriores es modificar adecuadamente el prompt para que se encuentre en español:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_prompt_template = \"\"\"\n",
    "Eres un asistente para un humano, alimentado por un modelo de lenguaje grande entrenado por OpenAI.\n",
    "\n",
    "Estás diseñado para poder ayudar con una amplia gama de tareas, desde responder preguntas simples hasta proporcionar explicaciones detalladas y participar en discusiones sobre una amplia variedad de temas. Como modelo de lenguaje, puedes generar texto similar al humano basado en la entrada que recibes, lo que te permite participar en conversaciones con un tono natural y proporcionar respuestas coherentes y relevantes al tema en cuestión.\n",
    "\n",
    "Estás constantemente aprendiendo y mejorando, y tus capacidades están en constante evolución. Puedes procesar y entender grandes cantidades de texto y utilizar este conocimiento para ofrecer respuestas precisas e informativas a una amplia variedad de preguntas. Tienes acceso a información personalizada proporcionada por el humano en la sección de Contexto a continuación. Además, puedes generar tu propio texto basado en la entrada que recibes, lo que te permite participar en discusiones y proporcionar explicaciones y descripciones sobre una amplia variedad de temas.\n",
    "\n",
    "En general, eres una herramienta poderosa que puede ayudar con una amplia gama de tareas y proporcionar información valiosa sobre una amplia variedad de temas. Ya sea que el humano necesite ayuda con una pregunta específica o simplemente quiera tener una conversación sobre un tema en particular, estás aquí para ayudar.\n",
    "\n",
    "Contexto:\n",
    "{entities}\n",
    "\n",
    "Conversación actual:\n",
    "{history}\n",
    "Última línea:\n",
    "Humano: {input}\n",
    "Tú:\n",
    "\"\"\"\n",
    "\n",
    "entity_prompt = PromptTemplate.from_template(entity_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationEntityMemory\n",
    "\n",
    "entity_memory = ConversationEntityMemory(llm=llm)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    verbose=True,\n",
    "    prompt=entity_prompt,\n",
    "    memory=entity_memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Jorge e Ignacio estan trabajando en un proyecto para Nielsen. Jorge es economista e Ignacio es ingeniero\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.memory.entity_store.store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Que tipo de empresa es Nielsen?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Jorge tiene el sueño de escalar el monte Everest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.memory.entity_store.store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Memoria basada con un grafo de conocimiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - `ConversationKGMemory`\n",
    "\n",
    "En este caso utilizamos el modelo va generando un pseudo grafo de conocimiento (en formato de texto) donde va almacenando la informacion relevante de la conversación, centrandose sobretodo en las diferentes entidades.\n",
    "\n",
    "Vamos a utilizar el template que hicimos anteriormente pero sin la variable `entities`, ya que no es utilizada por `ConversationKGMemory`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_prompt_template = \"\"\"\n",
    "Eres un asistente para un humano, alimentado por un modelo de lenguaje grande entrenado por OpenAI.\n",
    "\n",
    "Estás diseñado para poder ayudar con una amplia gama de tareas, desde responder preguntas simples hasta proporcionar explicaciones detalladas y participar en discusiones sobre una amplia variedad de temas. Como modelo de lenguaje, puedes generar texto similar al humano basado en la entrada que recibes, lo que te permite participar en conversaciones con un tono natural y proporcionar respuestas coherentes y relevantes al tema en cuestión.\n",
    "\n",
    "Estás constantemente aprendiendo y mejorando, y tus capacidades están en constante evolución. Puedes procesar y entender grandes cantidades de texto y utilizar este conocimiento para ofrecer respuestas precisas e informativas a una amplia variedad de preguntas. Tienes acceso a información personalizada proporcionada por el humano en la sección de Contexto a continuación. Además, puedes generar tu propio texto basado en la entrada que recibes, lo que te permite participar en discusiones y proporcionar explicaciones y descripciones sobre una amplia variedad de temas.\n",
    "\n",
    "En general, eres una herramienta poderosa que puede ayudar con una amplia gama de tareas y proporcionar información valiosa sobre una amplia variedad de temas. Ya sea que el humano necesite ayuda con una pregunta específica o simplemente quiera tener una conversación sobre un tema en particular, estás aquí para ayudar.\n",
    "\n",
    "Contexto:\n",
    "\n",
    "Conversación actual:\n",
    "{history}\n",
    "Última línea:\n",
    "Humano: {input}\n",
    "Tú:\n",
    "\"\"\"\n",
    "\n",
    "kg_prompt = PromptTemplate.from_template(kg_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationKGMemory\n",
    "\n",
    "kg_memory = ConversationKGMemory(llm=llm)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    verbose=True, \n",
    "    prompt=kg_prompt,\n",
    "    memory=kg_memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Jorge e Ignacio estan trabajando en un proyecto para Nielsen. Jorge es economista e Ignacio es ingeniero\")\n",
    "conversation.predict(input=\"Que tipo de empresa es Nielsen?\")\n",
    "conversation.predict(input=\"Jorge tiene el sueño de escalar el monte Everest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Que sabes de Ignacio?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_memory.kg.get_triples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta funcionalidad es similar a la que ofrece `ConversationEntityMemory`, pero organiza la informacion en forma de tripletas de un grafo de conocimiento.\n",
    "\n",
    "Además de ofrecernos la posibilidad de ver el grafo de conocimiento que el LLM va generando, LangChain nos ofrece metodos para interactuar con el mismo:\n",
    "* Extraccion de entidades\n",
    "* Extraccion de tripletas\n",
    "\n",
    "Pero mi experiencia personal es que no funciona muy bien, ya que no está 100% relacionado con el grafo que tiene en memoria por lo que resulta en un comportamiento extraño. Mi consejo en este caso seria hacer algo adhoc en vez de utilizar estos métodos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_memory.get_current_entities({\"input\":\"Que sabes de Ignacio, Nielsen y Google?\"}) # Extrae entidades de una frase, pero no tiene porque estar relacionado con su KG interno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_memory.get_knowledge_triplets({\"input\":\"Que sabes de Ignacio, Nielsen y Google?\"})# Extrae tripletas de una frase, pero no funciona muy bien"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Memoria basada en bases de datos vectoriales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - `VectorStoreRetrieverMemory`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`VectorStoreRetrieverMemory` almacena recuerdos en un VectorDB y consulta los documentos más \"relevantes\" principales cada vez que se llama. Esto difiere de la mayoría de las otras clases de memoria en que no sigue explícitamente el orden de las interacciones.\n",
    "\n",
    "En este caso, los \"documentos\" son fragmentos anteriores de la conversación. Esto puede ser útil para referirse a piezas relevantes de información que la IA recibió anteriormente en la conversación.\n",
    "\n",
    "```python\n",
    "pip install faiss-gpu # For CUDA 7.5+ Supported GPU's.\n",
    "# OR\n",
    "pip install faiss-cpu # For CPU Installation\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=openai.api_base, \n",
    "    azure_deployment=embeddings_engine, \n",
    "    openai_api_key=config.get(\"OPENAI_API_KEY\", \"\"), \n",
    "    openai_api_version=openai.api_version\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Esto es una prueba\"\n",
    "\n",
    "query_result = embeddings.embed_query(text)\n",
    "\n",
    "print(len(query_result)) # Devuelve un vector de dimension 1536\n",
    "# query_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar estos embeddings con una Base de datos vectorial como [FAISS](https://github.com/facebookresearch/faiss):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "\n",
    "embedding_size = 1536 # Dimensions of the OpenAIEmbeddings\n",
    "index = faiss.IndexFlatL2(embedding_size)\n",
    "vectorstore = FAISS(embeddings, index, InMemoryDocstore({}), {})\n",
    "\n",
    "vector_retriever = vectorstore.as_retriever(search_kwargs=dict(k=2)) # Devolvemos K documentos\n",
    "vector_memory  = VectorStoreRetrieverMemory(retriever=vector_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_memory.save_context({\"input\": \"Mi comida favorita es la pizza\"}, {\"output\": \"Gran elección!\"})\n",
    "vector_memory.save_context({\"input\": \"Todas las tarde juego al fútbol\"}, {\"output\": \"El fútbol es muy entretenido.\"})\n",
    "vector_memory.save_context({\"input\": \"No me gusta el golf\"}, {\"output\": \"...\"})\n",
    "vector_memory.save_context({\"input\": \"Mi ciudad favorita es Paris\"}, {\"output\": \"Genial\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vector_memory.load_memory_variables({\"prompt\": \"Que deporte me aconsejas para ver en la tele?\"})[\"history\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora veamos como usarlo con un cadena coversacional. Para ello, vamos a modificar levemente el prompt de tal forma que el modelo utilice los \"documentos\" recuperados para basar su respuesta si lo considera adecuado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_prompt_template = \"\"\"\n",
    "Lo siguiente es una conversación amistosa entre un humano y una inteligencia artificial. La IA es conversadora y proporciona muchos detalles específicos de su contexto. Si la IA no conoce la respuesta a una pregunta, dice honestamente que no lo sabe.\n",
    "\n",
    "Interacciones previas en la conversación que son relevantes:\n",
    "{history}\n",
    "\n",
    "(No necesitas utilizar estas interacciones previas para basar tu respuesta si no lo consideras relevante)\n",
    "\n",
    "Conversación actual:\n",
    "Humano: {input}\n",
    "IA:\n",
    "\"\"\"\n",
    "\n",
    "vector_prompt = PromptTemplate.from_template(vector_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    prompt=vector_prompt,\n",
    "    memory=vector_memory,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"¿Te gusta el deporte?\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siguiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente lección, veremos que es un agente, como utilizarlo, y sus principales usos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
