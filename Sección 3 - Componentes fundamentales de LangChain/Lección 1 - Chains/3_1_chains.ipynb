{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains\n",
    "\n",
    "**Sumario**\n",
    "\n",
    "1. Introducción\n",
    "   1. ¿Por qué necesitamos cadenas cuando usamos LLMs?\n",
    "   2. Diferentes maneras de llamar a las cadenas\n",
    "   3. ¿Cómo añadir memoria a las cadenas?\n",
    "   4. ¿Cómo depurar las cadenas?\n",
    "<br></br>\n",
    "2. Cadenas simples\n",
    "   1. `LLMChain`\n",
    "   2. `TransformChain`\n",
    "<br></br>\n",
    "1. Cadenas secuenciales\n",
    "   1. `SimpleSequentialChain`\n",
    "   2. `SequentialChain`\n",
    "<br></br>\n",
    "2. Cadenas de enrutamiento\n",
    "   1. `LLMRouterChain`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from langchain.llms import AzureOpenAI\n",
    "\n",
    "# Lee la clave de API desde el archivo de configuración\n",
    "with open('config.txt') as f:\n",
    "    config = dict(line.strip().split('=') for line in f)\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = \"https://gpt3tests.openai.azure.com/\"\n",
    "openai.api_version = \"2022-12-01\"\n",
    "openai.api_key = config.get(\"OPENAI_API_KEY\", \"\")\n",
    "\n",
    "# Nombre del despliegue en mi Azure OpenAI Studio is \"Davinci003\", el modelo es \"text-davinci-003\"\n",
    "engine = \"Davinci003\"\n",
    "model = \"text-davinci-003\"\n",
    "openai_api_version = \"2023-12-01\" \n",
    "\n",
    "# Nombre del despliegue en mi Azure OpenAI Studio para el modelo de embeddings es \"TextEmbeddingAda002\"\n",
    "embeddings_engine = \"TextEmbeddingAda002\"\n",
    "\n",
    "max_tokens = 1000\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    azure_endpoint=openai.api_base, \n",
    "    azure_deployment=engine, \n",
    "    openai_api_key=config.get(\"OPENAI_API_KEY\", \"\"), \n",
    "    openai_api_version=openai.api_version,\n",
    "    # temperature=0, # Podemos poner la temperatura a 0 si queremos reducir la variabilidad de las respuestas\n",
    ")\n",
    "llm.openai_api_base = openai.api_base \n",
    "llm.max_tokens = max_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las cadenas nos permiten fusionar varios componentes en una aplicación unificada. Por ejemplo, podemos formar una cadena que incorpore la entrada del usuario, aplique un [`PromptTemplate`](https://python.langchain.com/en/latest/modules/prompts/prompt_templates.html) para darle formato y posteriormente transfiera la respuesta formateada a un modelo de lenguaje grande (LLM, por sus siglas en inglés). Al combinar varias cadenas o integrarlas con otros componentes, podemos construir cadenas aún más intrincadas.\n",
    "\n",
    "La cadena más simple de estas es la [`LLMChain`](https://python.langchain.com/en/latest/modules/chains/generic/llm_chain.html). Funciona de la siguiente manera:\n",
    "1. Toma la entrada del usuario.\n",
    "2. La pasa al primer elemento de la cadena (es decir, un [`PromptTemplate`](https://python.langchain.com/en/latest/modules/prompts/prompt_templates.html)) para dar formato a la entrada y generar un prompt específico.\n",
    "3. El prompt formateado se pasa al siguiente (y último) elemento de la cadena, es decir, un LLM.\n",
    "\n",
    "Comenzaremos \"creando\" nuestro LLM, que será una instancia de GPT 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Creamos el template para nuestro prompt\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"producto\"],\n",
    "    template=\"Cual es un buen nombre para una empresa de {producto} (devuelve un único ejemplo. Nada más)\"\n",
    ")\n",
    "\n",
    "# Creamos la cadena mas simple posible que nos permite interactuar con un LLM mediante un prompt\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Corremos la cadena\n",
    "print(llm_chain.run(\"bicicletas de montaña\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para evitar gastos inesperados, podemos usar la siguiente función que nos indica cuantos tokens estamos utilizando en cada llamada. Ésto es especialmente útil cuando usamos módulos mas avanzados como Agentes, los cuales pueden realizar múltiples llamadas a la API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "def count_tokens(chain, query):\n",
    "    with get_openai_callback() as cb:\n",
    "        result = chain.run(query)\n",
    "        print(f'{cb.total_tokens} tokens han sido utilizados')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count_tokens(llm_chain, \"bicicletas de montaña\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si tenemos múltiples variables, podemos introducirla con un diccionario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"producto\", \"numero\"],\n",
    "    template=\"Cual es un buen nombre para una empresa de {producto} (devuelve {numero} ejemplos. Nada más)\",\n",
    ")\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "query = {\n",
    "    'producto': \"bicicletas de montaña\",\n",
    "    'numero': \"3\"\n",
    "    }\n",
    "print(count_tokens(llm_chain, query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - Diferentes maneras de llamar a las cadenas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todas las clases que heredan de [`Chain`](https://api.python.langchain.com/en/latest/chains/langchain.chains.base.Chain.html?highlight=chain#langchain.chains.base.Chain) ofrecen diferentes maneras de ser utilizadas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 - `__call__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"Cuentame un chiste {tipo}\"\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate.from_template(prompt_template)\n",
    ")\n",
    "\n",
    "llm_chain(inputs={\"tipo\": \"de miedo\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos configurarlo para que solo nos devuelva el output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain(inputs={\"tipo\": \"de miedo\"}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 - `run()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si la cadena solo devuelve una cosa (i.e., solo tiene un elemento en su `output_keys`), podemos usar el método `run`, el cual devuelve un string en vez de un diccionario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm_chain solo tiene un elemento en output_keys, por lo que podemos usarlo con run\n",
    "llm_chain.output_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain.run({\"tipo\":\"de miedo\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 - ¿Cómo añadir memoria a las cadenas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Chain` admite tomar un objeto `BaseMemory` como su argumento `memory`, lo que permite que el objeto `Chain` persista datos a lo largo de múltiples llamadas. En otras palabras, convierte a `Chain` en un objeto con estado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "conversacion = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "print(conversacion.run(\"Responde brevemente. ¿Cuáles son los primeros 3 colores de un arcoíris? (Devuelve solo los nombres)\"))\n",
    "print(conversacion.run(\"¿Y los siguientes 4?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esencialmente, `BaseMemory` define una interfaz sobre cómo LangChain almacena la memoria. Permite la lectura de datos almacenados a través del método `load_memory_variables()` y el almacenamiento de nuevos datos mediante el método `save_context`.\n",
    "\n",
    "Veremos más sobre la memoria en las siguientes clases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 - ¿Cómo depurar las cadenas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede ser difícil depurar un objeto `Chain` únicamente desde su salida, ya que la mayoría de los objetos `Chain` involucran una cantidad considerable de preprocesamiento de la entrada del prompt y postprocesamiento de la salida del LLM. Establecer `verbose=True` imprimirá algunos estados internos del objeto `Chain` mientras se está ejecutando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferMemory(),\n",
    "    verbose=True\n",
    ")\n",
    "conversation.run(\"¿Qué es ChatGPT?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain agrega un texto por defecto a las prompts cuando utilizamos ciertas cadenas como `ConversationChain` para guiar al modelo de lenguaje. Si bien estamos utilizando el español como lenguaje, el texto se encuentra por defecto en inglés, lo cual podría confundir al modelo. Más adelante, veremos cómo podemos modificar estas prompts por defecto para que se alineen con nuestro idioma y objetivos específicos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "\n",
    "langchain.debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.run(\"¿Qué es ChatGPT?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain.debug = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Cadenas simples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - `LLMChain`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_3_1/llm_chain.png\" width=\"700\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Hemos visto ejemplos de la misma más arriba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - `TransformChain`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_3_1/transform_chain.png\" width=\"700\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Como ejemplo, si hemos lidiado con textos de entrada desordenados, lo cual podría resultar en un costo adicional debido a la tarificación por tokens en APIs como OpenAI, podemos crear una función personalizada para limpiar los espacios en nuestros textos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain.chains import TransformChain\n",
    "\n",
    "def transform_func(inputs: dict) -> dict:\n",
    "    text = inputs[\"text\"]\n",
    "    \n",
    "    # reemplaza múltiples saltos de línea y múltiples espacios con uno solo\n",
    "    text = re.sub(r'(\\r\\n|\\r|\\n){2,}', r'\\n', text)\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "\n",
    "    return {\"output_text\": text}\n",
    "\n",
    "clean_extra_spaces_chain = TransformChain(input_variables=[\"text\"], output_variables=[\"output_text\"], transform=transform_func)\n",
    "clean_extra_spaces_chain.run('Un texto aleatorio  con   espaciado irregular.\\n\\n\\n     Otro aquí   también.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este tipo de cadena no se suele usar por si sola, sino que suele ser combinada con otras cadenas como `LLMChain`. Ahora veremos como podemos combinar diferentes cadenas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Cadenas secuenciales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La idea detrás de las cadenas secuenciales es combinar varias cadenas donde **la salida de una cadena es la entrada de la siguiente**. Hay dos tipos de cadenas secuenciales:\n",
    "\n",
    "* `SimpleSequentialChain`: Una sola entrada/salida\n",
    "* `SequentialChain`: Múltiples entradas/salidas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - `SimpleSequentialChain`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_3_1/simple_sequential_chain.png\" width=\"700\"/></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Primera cadena\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"¿Cuál es el mejor nombre para describir a una empresa que fabrica {producto} (devuelve solo un ejemplo, nada más)?\"\n",
    ")\n",
    "first_chain = LLMChain(llm=llm, prompt=first_prompt)\n",
    "\n",
    "# Segunda cadena\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Escribe una descripción de 20 palabras en español para la siguiente empresa: {nombre_empresa}\"\n",
    ")\n",
    "second_chain = LLMChain(llm=llm, prompt=second_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_seq_chain = SimpleSequentialChain(chains=[first_chain, second_chain], verbose=True)\n",
    "print(count_tokens(simple_seq_chain, \"bicicletas de montaña\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - `SequentialChain`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SimpleSequentialChain` funciona bien cuando tenemos una sola entrada y una sola salida. Pero, ¿qué sucede cuando tenemos múltiples entradas y salidas?\n",
    "\n",
    "A modo de ejemplo, vamos a realizar los siguientes pasos:\n",
    "* Crear dos breves historias utilizando dos cadenas independientes, una en español y otra en inglés. Podríamos emplear dos modelos diferentes, por ejemplo.\n",
    "* Traducir la historia en inglés a español.\n",
    "* Evaluar cual de las dos historias es más entretenida.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_3_1/sequential_chain.png\" width=\"800\"/></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 - Crear breves historias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historia en español\n",
    "spanish_story_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Crea una breve cuento de 50 palabras sobre un {protagonista} en español:\"\n",
    ")\n",
    "spanish_story_chain = LLMChain(llm=llm, prompt=spanish_story_prompt, output_key=\"spanish_story\")\n",
    "\n",
    "# Historia en inglés\n",
    "english_story_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Crea una breve cuento de 50 palabras sobre un {protagonista} en inglés:\"\n",
    ")\n",
    "english_story_chain = LLMChain(llm=llm, prompt=english_story_prompt, output_key=\"english_story\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 - Traducir la historia en inglés a español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recibe como input la salida de la \"english_story_chain\"\n",
    "translate_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Traduce el siguiente cuento al español:\"\n",
    "    \"\\n\\n{english_story}\"\n",
    ")\n",
    "translate_chain = LLMChain(llm=llm, prompt=translate_prompt, output_key=\"translated_story\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 - Evaluar cual de las dos historias es más entretenida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recibe como inputs los dos cuentos en español (uno original y el otro la traduccion del de ingles)\n",
    "evaluate_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Otorga una puntuacion entre 1 y 10 a las siguientes cuentos segun su nivel de entretenimiento\"\n",
    "    \"\\n\\nCuento 1: {spanish_story}\\n\\nCuento 2: {translated_story}\"\n",
    ")\n",
    "evaluate_chain = LLMChain(llm=llm, prompt=evaluate_prompt, output_key=\"puntuacion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 - Crear la `SequentialChain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[spanish_story_chain, english_story_chain, translate_chain, evaluate_chain],\n",
    "    input_variables=[\"protagonista\"],\n",
    "    output_variables=[\"spanish_story\", \"english_story\", \"translated_story\", \"puntuacion\"],\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# \"Crea una breve cuento de 50 palabras sobre un {protagonista}\"\n",
    "protagonista = \"gato con botas\"\n",
    "overall_chain(protagonista)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Cadenas de enrutamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 - `LLMRouterChain`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Básicamente, generamos una cadena específica respaldada por un LLM que redirige a la subcadena adecuada al leer la pregunta y las descripciones de la plantilla de indicaciones.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_3_1/router_chain.png\" width=\"700\"/></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 - Crear templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asumiendo que el la cadena de enrutamiento funciona correctamente, pasaria a utilizar el template correspondiente para \"influenciar\" al LLM a comportarse lo mas cercano a la pregunta posible. Por ejemplo, si la pregunta es sobre matematicas, le ponemos un encabezado diciendo que sabe de matematicas ya que se ha mostrado experimentalmente que mejora la calidad de los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physics_template = \"\"\"Eres un profesor de biología muy inteligente. \\\n",
    "Eres excelente para responder preguntas sobre biología de manera concisa \\\n",
    "y fácil de entender. \\\n",
    "Cuando no sabes la respuesta a una pregunta, admites \\\n",
    "que no lo sabes.\n",
    "\n",
    "Aquí tienes una pregunta:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "math_template = \"\"\"Eres un matemático muy hábil. \\\n",
    "Eres excelente para responder preguntas de matemáticas. \\\n",
    "Eres tan bueno porque puedes descomponer \\\n",
    "problemas difíciles en sus partes componentes, \\\n",
    "responder a esas partes y luego unirlas \\\n",
    "para responder a la pregunta más amplia.\n",
    "\n",
    "Aquí tienes una pregunta:\n",
    "{input}\"\"\"\n",
    "\n",
    "history_template = \"\"\"Eres un historiador muy competente. \\\n",
    "Tienes un excelente conocimiento y comprensión de las personas, \\\n",
    "eventos y contextos en diferentes períodos históricos. \\\n",
    "Tienes la capacidad de pensar, reflexionar, debatir, discutir y \\\n",
    "evaluar el pasado. Tienes respeto por la evidencia histórica \\\n",
    "y la habilidad de utilizarla para respaldar tus explicaciones \\\n",
    "y juicios.\n",
    "\n",
    "Aquí tienes una pregunta:\n",
    "{input}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 - Información sobre los templates para el router"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para que el router redirija correctamente tiene que tener una descripcion de los diferentes templates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        \"nombre\": \"Biología\", \n",
    "        \"descripcion\": \"Bueno para responder preguntas sobre biología\", \n",
    "        \"prompt_template\": physics_template\n",
    "    },\n",
    "    {\n",
    "        \"nombre\": \"Matemáticas\", \n",
    "        \"descripcion\": \"Bueno para responder preguntas sobre matemáticas\", \n",
    "        \"prompt_template\": math_template\n",
    "    },\n",
    "    {\n",
    "        \"nombre\": \"Historia\", \n",
    "        \"descripcion\": \"Bueno para responder preguntas sobre historia\", \n",
    "        \"prompt_template\": history_template\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4 - Cadenas específicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a crear una cadena especifica para cada uno de los templates. Al igual que en casos anteriores, estamos usando siempre el mismo modelo (GPT-3) pero podriamos usar modelos diferentes para cada uno de los casos. Por ejemplo si trabajasemos con diferentes tipos de inputs (e.g., imagenes, formulas matematicas, etc.) donde tuviera sentido usar modelos especializados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"nombre\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain  \n",
    "    \n",
    "destinations = [f\"{p['nombre']}: {p['descripcion']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En caso de que no tenga claro que va con ninguna de los templates disponibles, puede utilizar el modelo por defecto sin ningun tipo de prompt especifico (e.g., matematicas, biologia, historia, etc.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.5 - Crear `LLMRouterChain`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya hemos creado las cadenas espec´ficias y les hemos asignado una descripción para que el modelo de enrutamiento sepa cual es la mejor cadena para cada caso.\n",
    "\n",
    "Ahora, tenemos que crear la cadena de enrutamiento. Sin embargo, tenemos un pequeño problema, y es que el template por defecto que utiliza `RouterChain` se encuentra en inglés. \n",
    "\n",
    "Ante esto tenemos dos posibilidades:\n",
    "\n",
    "* Trabajamos en inglés y traducimos los textos con una cadena secuencial como hemos visto anteriormente.\n",
    "* Modificamos el template para que el modelo de enrutamiento sepa que el texto le viene en español."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
    "\n",
    "print(MULTI_PROMPT_ROUTER_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_router_template = \"\"\"Given a raw text input to a \\\n",
    "language model select the model prompt best suited for the input. \\\n",
    "You will be given the names of the available prompts and a \\\n",
    "description of what the prompt is best suited for. \\\n",
    "You may also revise the original input if you think that revising\\\n",
    "it will ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
    "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
    "well suited for any of the candidate prompts.\n",
    "REMEMBER: \"next_inputs\" can just be the original input \\\n",
    "if you don't think any modifications are needed.\n",
    "\n",
    "IMPORTANT: Both inputs and candidate prompts are in Spanish\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_router_template = custom_router_template.format(\n",
    "    destinations=destinations_str\n",
    ")\n",
    "\n",
    "router_prompt = PromptTemplate(\n",
    "    template=custom_router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.6 - Combinar cadenas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder combinar las cadenas necesitamos una ruta que agrupa todos los componentes anteriores que es la `MultiPromptChain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = MultiPromptChain(router_chain=router_chain, \n",
    "                         destination_chains=destination_chains, \n",
    "                         default_chain=default_chain, \n",
    "                         verbose=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.run(\"¿Qué dice el teorema de Fermat?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain.debug = True\n",
    "chain.run(\"¿En que año cayó el muro de Berlin?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siguiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente lección, veremos como podemos añadir memoria a nuestras cadenas para que el modelo recuerde las interacciones pasadas y/o datos relevantes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
